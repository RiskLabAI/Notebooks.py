{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data for Heterogeneous Hedged Random Forest\n",
    "\n",
    "This notebook demonstrates the application of the **Heterogeneous Hedged Random Forest** (HH-RF) model in comparison with the **Standard Random Forest** (RF) and **Hedged Random Forest** (H-RF) models on synthetic data that exhibits heterogeneous regimes. The experiment is designed to showcase the model's performance across distinct data regimes based on a conditioning variable $z$.\n",
    "\n",
    "## Data Generation and Mathematical Setup\n",
    "\n",
    "The dataset consists of $n = 1000$ samples with $d = 5$ predictor variables, denoted as $X = [X_1, X_2, \\dots, X_5]$, and a conditioning variable $z$ which determines the underlying regime. The target variable $y$ is defined as a piecewise function of $X$ and $z$:\n",
    "\n",
    "$$\n",
    "y = f(X, z) + \\epsilon(z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $f(X, z)$ is a piecewise function that describes different relationships between $X$ and $y$ depending on $z$:\n",
    "\n",
    "$$\n",
    "f(X, z) =\n",
    "\\begin{cases}\n",
    "    2X_1 - X_2 + 3, & \\text{if } z < 0.3 \\quad \\text{(Regime 1: Linear)} \\\\\n",
    "    \\sin(2\\pi X_1) + X_2^2 - 1, & \\text{if } 0.3 \\leq z < 0.6 \\quad \\text{(Regime 2: Non-linear)} \\\\\n",
    "    0.5X_1 - 2X_3 + \\eta, & \\text{if } z \\geq 0.6 \\quad \\text{(Regime 3: Noisy)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, $\\eta \\sim \\mathcal{N}(0, 2^2)$ is a high-noise term in Regime 3.\n",
    "\n",
    "- The noise $\\epsilon(z)$ is also heterogeneous and is defined as:\n",
    "\n",
    "$$\n",
    "\\epsilon(z) =\n",
    "\\begin{cases}\n",
    "    \\mathcal{N}(0, 0.5^2), & \\text{if } z < 0.3 \\quad \\text{(Low Noise)} \\\\\n",
    "    \\mathcal{N}(0, 1^2), & \\text{if } 0.3 \\leq z < 0.6 \\quad \\text{(Medium Noise)} \\\\\n",
    "    \\mathcal{N}(0, 2^2), & \\text{if } z \\geq 0.6 \\quad \\text{(High Noise)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus, the target $y$ is a noisy and heterogeneous function of $X$ that varies depending on the conditioning variable $z$.\n",
    "\n",
    "### Visualizing the Synthetic Data\n",
    "\n",
    "Let's visualize the generated synthetic data where the data points are color-coded based on the conditioning variable $z$ to indicate which regime they belong to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples and dimensions\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "# Generate random features (X)\n",
    "X = np.random.uniform(-1, 1, size=(n_samples, n_features))\n",
    "\n",
    "# Generate conditioning variable (z)\n",
    "z = np.random.uniform(0, 1, size=n_samples)\n",
    "\n",
    "# Define piecewise function f(X, z)\n",
    "def f(X, z):\n",
    "    # Regime 1: z < 0.3 -> Linear relationship\n",
    "    linear = X[:, 0] * 2 + X[:, 1] * (-1) + 3\n",
    "    # Regime 2: 0.3 <= z < 0.6 -> Non-linear relationship\n",
    "    nonlinear = np.sin(2 * np.pi * X[:, 0]) + X[:, 1]**2 - 1\n",
    "    # Regime 3: z >= 0.6 -> Mixed relationship with high noise\n",
    "    noisy = X[:, 0] * 0.5 + X[:, 2] * (-2) + np.random.normal(0, 2, size=X.shape[0])\n",
    "\n",
    "    # Combine regimes based on z\n",
    "    y = np.where(z < 0.3, linear,\n",
    "                 np.where(z < 0.6, nonlinear, noisy))\n",
    "    return y\n",
    "\n",
    "# Generate target variable y\n",
    "y = f(X, z)\n",
    "\n",
    "# Add heterogeneous noise\n",
    "noise = np.where(z < 0.3, np.random.normal(0, 0.5, size=n_samples),  # Low noise\n",
    "                 np.where(z < 0.6, np.random.normal(0, 1, size=n_samples),  # Medium noise\n",
    "                          np.random.normal(0, 2, size=n_samples)))  # High noise\n",
    "y += noise\n",
    "\n",
    "# Visualize data\n",
    "# Define clusters based on z values\n",
    "cluster_1 = z < 0.3  # Cluster 1: z < 0.3\n",
    "cluster_2 = (z >= 0.3) & (z < 0.6)  # Cluster 2: 0.3 <= z < 0.6\n",
    "cluster_3 = z >= 0.6  # Cluster 3: z >= 0.6\n",
    "\n",
    "# Assign colors to the clusters\n",
    "colors = np.zeros_like(z, dtype=int)\n",
    "colors[cluster_1] = 0  # Cluster 1: Color 0 (e.g., red)\n",
    "colors[cluster_2] = 1  # Cluster 2: Color 1 (e.g., blue)\n",
    "colors[cluster_3] = 2  # Cluster 3: Color 2 (e.g., green)\n",
    "\n",
    "# Define the color map\n",
    "cmap = plt.get_cmap('RdYlBu')  # Or any other color map\n",
    "\n",
    "# Plot the data with colors representing the clusters\n",
    "plt.scatter(z, y, c=colors, cmap=cmap, alpha=0.7, edgecolors='k', label=\"Target Variable (y)\")\n",
    "plt.xlabel(\"Conditioning Variable (z)\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Synthetic Data: Heterogeneous Regimes with Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Plot\n",
    "\n",
    "The scatter plot above shows the synthetic data, with each data point colored according to the regime it belongs to based on the value of $z$. The three distinct colors represent the different regimes with varying relationships and noise levels:\n",
    "\n",
    "- Regime 1: Linear relationship with low noise ($z < 0.3$).\n",
    "- Regime 2: Non-linear relationship with medium noise ($0.3 \\leq z < 0.6$).\n",
    "- Regime 3: Mixed relationship with high noise ($z \\geq 0.6$).\n",
    "\n",
    "## Model Training and Evaluation\n",
    "\n",
    "We now evaluate the performance of the three models: Standard Random Forest (RF), Hedged Random Forest (H-RF), and Heterogeneous Hedged Random Forest (HH-RF). The models are trained on the features $X$ and target $y$, and evaluated on the test set.\n",
    "\n",
    "### MSE, MAE, and $R^2$ Evaluation\n",
    "\n",
    "We use three performance metrics: **Mean Squared Error (MSE)**, **Mean Absolute Error (MAE)**, and **$R^2$ Score**.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Let's train and evaluate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hedged_random_forests import HedgedRandomForestRegressor\n",
    "from heterogeneous_hedged_random_forests import HeterogeneousHedgedRandomForestRegressor\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, z, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit and evaluate Vanilla RF\n",
    "vanilla_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "vanilla_rf.fit(X_train, y_train)\n",
    "rf_preds = vanilla_rf.predict(X_test)\n",
    "rf_mse = np.mean((y_test - rf_preds) ** 2)\n",
    "\n",
    "# Fit and evaluate H-RF\n",
    "h_rf = HedgedRandomForestRegressor(n_estimators=100, random_state=42)\n",
    "h_rf.fit(X_train, y_train)\n",
    "h_rf_preds = h_rf.predict(X_test)\n",
    "h_rf_mse = np.mean((y_test - h_rf_preds) ** 2)\n",
    "\n",
    "# Fit and evaluate HH-RF\n",
    "hh_rf = HeterogeneousHedgedRandomForestRegressor(n_estimators=100, n_partition=3, random_state=42)\n",
    "hh_rf.fit(X_train, y_train, z_train.reshape(-1, 1))\n",
    "hh_rf_preds = hh_rf.predict(X_test, z_test.reshape(-1, 1))\n",
    "hh_rf_mse = np.mean((y_test - hh_rf_preds) ** 2)\n",
    "\n",
    "# Print results\n",
    "print(f\"Vanilla RF MSE: {rf_mse}\")\n",
    "print(f\"H-RF MSE: {h_rf_mse}\")\n",
    "print(f\"HH-RF MSE: {hh_rf_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics for Each Model\n",
    "\n",
    "The following table shows the **MSE**, **MAE**, and **$R^2$** scores for the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "metrics = {\n",
    "    'Model': ['Standard RF', 'Hedged RF', 'Heterogeneous Hedged RF'],\n",
    "    'MSE': [rf_mse, h_rf_mse, hh_rf_mse],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_test, rf_preds),\n",
    "        mean_absolute_error(y_test, h_rf_preds),\n",
    "        mean_absolute_error(y_test, hh_rf_preds)\n",
    "    ],\n",
    "    'R²': [\n",
    "        r2_score(y_test, rf_preds),\n",
    "        r2_score(y_test, h_rf_preds),\n",
    "        r2_score(y_test, hh_rf_preds)\n",
    "    ]\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below summarizes the performance of each model:\n",
    "\n",
    "| Model                     | MSE        | MAE        | $R^2$  |\n",
    "|---------------------------|------------|------------|------------|\n",
    "| **Standard RF**            | 7.0339     | 2.1511     | 0.0041     |\n",
    "| **Hedged RF**              | 6.9409     | 2.1070     | 0.0172     |\n",
    "| **Heterogeneous Hedged RF**| 6.8365     | 2.0866     | 0.0320     |\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The **Heterogeneous Hedged Random Forest** (HH-RF) outperforms both the **Standard RF** and **Hedged RF** models in terms of MSE, MAE, and $R^2$, showing its ability to adapt to the heterogeneous nature of the synthetic dataset. By incorporating conditional partitioning and adaptive hedging, HH-RF is able to capture regime-specific relationships and noise structures, leading to improved predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Quadratic Inverse Shrinkage for the Errors Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of samples and dimensions\n",
    "n_samples = 5000\n",
    "n_features = 5\n",
    "\n",
    "# Generate random features (X)\n",
    "X = np.random.uniform(-1, 1, size=(n_samples, n_features))\n",
    "\n",
    "# Generate conditioning variable (z)\n",
    "z = np.random.uniform(0, 1, size=n_samples)\n",
    "\n",
    "# Define piecewise function f(X, z)\n",
    "def f(X, z):\n",
    "    # Regime 1: z < 0.3 -> Linear relationship\n",
    "    linear = X[:, 0] * 2 + X[:, 1] * (-1) + 3\n",
    "    # Regime 2: 0.3 <= z < 0.6 -> Non-linear relationship\n",
    "    nonlinear = np.sin(2 * np.pi * X[:, 0]) + X[:, 1]**2 - 1\n",
    "    # Regime 3: z >= 0.6 -> Mixed relationship with high noise\n",
    "    noisy = X[:, 0] * 0.5 + X[:, 2] * (-2) + np.random.normal(0, 2, size=X.shape[0])\n",
    "\n",
    "    # Combine regimes based on z\n",
    "    y = np.where(z < 0.3, linear,\n",
    "                 np.where(z < 0.6, nonlinear, noisy))\n",
    "    return y\n",
    "\n",
    "# Generate target variable y\n",
    "y = f(X, z)\n",
    "\n",
    "# Add heterogeneous noise\n",
    "noise = np.where(z < 0.3, np.random.normal(0, 0.5, size=n_samples),  # Low noise\n",
    "                 np.where(z < 0.6, np.random.normal(0, 1, size=n_samples),  # Medium noise\n",
    "                          np.random.normal(0, 2, size=n_samples)))  # High noise\n",
    "y += noise\n",
    "\n",
    "# Visualize data\n",
    "# Define clusters based on z values\n",
    "cluster_1 = z < 0.3  # Cluster 1: z < 0.3\n",
    "cluster_2 = (z >= 0.3) & (z < 0.6)  # Cluster 2: 0.3 <= z < 0.6\n",
    "cluster_3 = z >= 0.6  # Cluster 3: z >= 0.6\n",
    "\n",
    "# Assign colors to the clusters\n",
    "colors = np.zeros_like(z, dtype=int)\n",
    "colors[cluster_1] = 0  # Cluster 1: Color 0 (e.g., red)\n",
    "colors[cluster_2] = 1  # Cluster 2: Color 1 (e.g., blue)\n",
    "colors[cluster_3] = 2  # Cluster 3: Color 2 (e.g., green)\n",
    "\n",
    "# Define the color map\n",
    "cmap = plt.get_cmap('RdYlBu')  # Or any other color map\n",
    "\n",
    "# Plot the data with colors representing the clusters\n",
    "plt.scatter(z, y, c=colors, cmap=cmap, alpha=0.7, edgecolors='k', label=\"Target Variable (y)\")\n",
    "plt.xlabel(\"Conditioning Variable (z)\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Synthetic Data: Heterogeneous Regimes with Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hedged_random_forests import HedgedRandomForestRegressor\n",
    "from heterogeneous_hedged_random_forests import HeterogeneousHedgedRandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, z, test_size=0.3, random_state=42)\n",
    "\n",
    "# Fit and evaluate Vanilla RF\n",
    "vanilla_rf = RandomForestRegressor(n_estimators=500, random_state=42)\n",
    "vanilla_rf.fit(X_train, y_train)\n",
    "rf_preds = vanilla_rf.predict(X_test)\n",
    "rf_mse = np.mean((y_test - rf_preds) ** 2)\n",
    "\n",
    "# Fit and evaluate H-RF with shrinkage='ledoit_wolf'\n",
    "h_rf = HedgedRandomForestRegressor(n_estimators=500, random_state=42, shrinkage='ledoit_wolf') \n",
    "h_rf.fit(X_train, y_train)\n",
    "h_rf_preds = h_rf.predict(X_test)\n",
    "h_rf_mse = np.mean((y_test - h_rf_preds) ** 2)\n",
    "\n",
    "# Fit and evaluate HH-RF with shrinkage='ledoit_wolf'\n",
    "hh_rf = HeterogeneousHedgedRandomForestRegressor(n_estimators=500, n_partition=3, random_state=42, shrinkage='ledoit_wolf') \n",
    "hh_rf.fit(X_train, y_train, z_train.reshape(-1, 1))\n",
    "hh_rf_preds = hh_rf.predict(X_test, z_test.reshape(-1, 1))\n",
    "hh_rf_mse = np.mean((y_test - hh_rf_preds) ** 2)\n",
    "\n",
    "# Fit and evaluate H-RF with shrinkage='quadratic_inverse'\n",
    "h_rf_quadratic = HedgedRandomForestRegressor(n_estimators=500, random_state=42, shrinkage='quadratic_inverse') \n",
    "h_rf_quadratic.fit(X_train, y_train)\n",
    "h_rf_quadratic_preds = h_rf_quadratic.predict(X_test)\n",
    "h_rf_quadratic_mse = np.mean((y_test - h_rf_quadratic_preds) ** 2)\n",
    "\n",
    "# Fit and evaluate HH-RF with shrinkage='quadratic_inverse'\n",
    "hh_rf_quadratic = HeterogeneousHedgedRandomForestRegressor(n_estimators=500, n_partition=3, random_state=42, shrinkage='quadratic_inverse') \n",
    "hh_rf_quadratic.fit(X_train, y_train, z_train.reshape(-1, 1))\n",
    "hh_rf_quadratic_preds = hh_rf_quadratic.predict(X_test, z_test.reshape(-1, 1))\n",
    "hh_rf_quadratic_mse = np.mean((y_test - hh_rf_quadratic_preds) ** 2)\n",
    "\n",
    "# Prepare metrics\n",
    "metrics = {\n",
    "    'Model': ['Standard RF', 'Hedged RF (ledoit)', 'Heterogeneous Hedged RF (ledoit)', 'Hedged RF (quadratic)', 'Heterogeneous Hedged RF (quadratic)'],\n",
    "    'MSE': [rf_mse, h_rf_mse, hh_rf_mse, h_rf_quadratic_mse, hh_rf_quadratic_mse],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_test, rf_preds),\n",
    "        mean_absolute_error(y_test, h_rf_preds),\n",
    "        mean_absolute_error(y_test, hh_rf_preds),\n",
    "        mean_absolute_error(y_test, h_rf_quadratic_preds),\n",
    "        mean_absolute_error(y_test, hh_rf_quadratic_preds)\n",
    "    ],\n",
    "    'R²': [\n",
    "        r2_score(y_test, rf_preds),\n",
    "        r2_score(y_test, h_rf_preds),\n",
    "        r2_score(y_test, hh_rf_preds),\n",
    "        r2_score(y_test, h_rf_quadratic_preds),\n",
    "        r2_score(y_test, hh_rf_quadratic_preds)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "| **Model**                                         | **MSE**   | **MAE**   | **R²**    |\n",
    "|---------------------------------------------------|-----------|-----------|-----------|\n",
    "| Standard RF                                       | 7.1067    | 2.1825    | -0.0602   |\n",
    "| Hedged RF (Linear)                                | 7.1804    | 2.1937    | -0.0712   |\n",
    "| Heterogeneous Hedged RF (Linear)                  | 7.0888    | 2.1625    | -0.0575   |\n",
    "| Hedged RF (Non Linear)                            | 7.1724    | 2.1904    | -0.0700   |\n",
    "| Heterogeneous Hedged RF (Non Linear)              | 7.0786    | 2.1650    | -0.0560   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM \\& Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from hedged_random_forests import HedgedRandomForestRegressor\n",
    "from heterogeneous_hedged_random_forests import HeterogeneousHedgedRandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define piecewise function f(X, z)\n",
    "def f(X, z):\n",
    "    # Regime 1: z < 0.3 -> Linear relationship\n",
    "    linear = X[:, 0] * 2 + X[:, 1] * (-1) + 3\n",
    "    # Regime 2: 0.3 <= z < 0.6 -> Non-linear relationship\n",
    "    nonlinear = np.sin(2 * np.pi * X[:, 0]) + X[:, 1]**2 - 1\n",
    "    # Regime 3: z >= 0.6 -> Mixed relationship with high noise\n",
    "    noisy = X[:, 0] * 0.5 + X[:, 2] * (-2) + np.random.normal(0, 2, size=X.shape[0])\n",
    "\n",
    "    # Combine regimes based on z\n",
    "    y = np.where(z < 0.3, linear,\n",
    "                 np.where(z < 0.6, nonlinear, noisy))\n",
    "    return y\n",
    "\n",
    "# Function to calculate mean and confidence intervals\n",
    "def calculate_stats(values):\n",
    "    mean = np.mean(values)\n",
    "    stderr = np.std(values, ddof=1) / np.sqrt(len(values))\n",
    "    margin = 1.96 * stderr  # Assuming normality\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "# Perform the experiment for multiple random seeds\n",
    "n_seeds = 100\n",
    "mse_results = {\n",
    "    \"Vanilla RF\": [],\n",
    "    \"H-RF (Linear)\": [],\n",
    "    \"H-RF (Non Linear)\": [],\n",
    "    \"HH-RF (Linear)\": [],\n",
    "    \"HH-RF (Non Linear)\": []\n",
    "}\n",
    "\n",
    "for seed in tqdm(range(n_seeds), desc=\"Running experiments\"):\n",
    "    # Generate synthetic data\n",
    "    np.random.seed(seed)\n",
    "    n_samples, n_features = 5000, 5\n",
    "    X = np.random.uniform(-1, 1, size=(n_samples, n_features))\n",
    "    z = np.random.uniform(0, 1, size=n_samples)\n",
    "    y = f(X, z)\n",
    "    \n",
    "    # Add heterogeneous noise\n",
    "    noise = np.where(z < 0.3, np.random.normal(0, 0.5, size=n_samples),  # Low noise\n",
    "                    np.where(z < 0.6, np.random.normal(0, 1, size=n_samples),  # Medium noise\n",
    "                            np.random.normal(0, 2, size=n_samples)))  # High noise\n",
    "    \n",
    "    y += noise\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, z, test_size=0.3, random_state=seed)\n",
    "\n",
    "    # Vanilla Random Forest\n",
    "    vanilla_rf = RandomForestRegressor(n_estimators=500, random_state=seed, n_jobs=16)\n",
    "    vanilla_rf.fit(X_train, y_train)\n",
    "    rf_preds = vanilla_rf.predict(X_test)\n",
    "    mse_results[\"Vanilla RF\"].append(np.mean((y_test - rf_preds) ** 2))\n",
    "\n",
    "    # H-RF Ledoit-Wolf\n",
    "    h_rf = HedgedRandomForestRegressor(n_estimators=500, random_state=seed, shrinkage='ledoit_wolf', n_jobs=16)\n",
    "    h_rf.fit(X_train, y_train)\n",
    "    h_rf_preds = h_rf.predict(X_test)\n",
    "    mse_results[\"H-RF (Linear)\"].append(np.mean((y_test - h_rf_preds) ** 2))\n",
    "\n",
    "    # H-RF Quadratic-Inverse\n",
    "    h_rf_quadratic = HedgedRandomForestRegressor(n_estimators=500, random_state=seed, shrinkage='quadratic_inverse', n_jobs=16)\n",
    "    h_rf_quadratic.fit(X_train, y_train)\n",
    "    h_rf_quadratic_preds = h_rf_quadratic.predict(X_test)\n",
    "    mse_results[\"H-RF (Non Linear)\"].append(np.mean((y_test - h_rf_quadratic_preds) ** 2))\n",
    "\n",
    "    # HH-RF Ledoit-Wolf\n",
    "    hh_rf = HeterogeneousHedgedRandomForestRegressor(n_estimators=500, n_partition=3, random_state=seed, shrinkage='ledoit_wolf', n_jobs=16)\n",
    "    hh_rf.fit(X_train, y_train, z_train.reshape(-1, 1))\n",
    "    hh_rf_preds = hh_rf.predict(X_test, z_test.reshape(-1, 1))\n",
    "    mse_results[\"HH-RF (Linear)\"].append(np.mean((y_test - hh_rf_preds) ** 2))\n",
    "\n",
    "    # HH-RF Quadratic-Inverse\n",
    "    hh_rf_quadratic = HeterogeneousHedgedRandomForestRegressor(n_estimators=500, n_partition=3, random_state=seed, shrinkage='quadratic_inverse', n_jobs=16)\n",
    "    hh_rf_quadratic.fit(X_train, y_train, z_train.reshape(-1, 1))\n",
    "    hh_rf_quadratic_preds = hh_rf_quadratic.predict(X_test, z_test.reshape(-1, 1))\n",
    "    mse_results[\"HH-RF (Non Linear)\"].append(np.mean((y_test - hh_rf_quadratic_preds) ** 2))\n",
    "\n",
    "# Calculate mean and confidence intervals\n",
    "stats = {}\n",
    "for model, mses in mse_results.items():\n",
    "    stats[model] = calculate_stats(mses)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_stats = pd.DataFrame(stats, index=[\"Mean\", \"Lower 95% CI\", \"Upper 95% CI\"]).T\n",
    "print(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a visually appealing boxplot for mse_results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(mse_results.values(), labels=mse_results.keys(), patch_artist=True)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Model MSE Results Across 100 Seeds\", fontsize=16)\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\", fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add colors to the boxplot for better differentiation\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'wheat', 'violet']\n",
    "for patch, color in zip(plt.gca().artists, colors):\n",
    "    patch.set_facecolor(color)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                | Mean      | Lower 95% CI | Upper 95% CI |\n",
    "|----------------------|-----------|--------------|--------------|\n",
    "| Vanilla RF           | 7.182372  | 7.138254     | 7.226490     |\n",
    "| H-RF (Linear)        | 7.353508  | 7.307669     | 7.399348     |\n",
    "| H-RF (Non Linear)    | 7.344688  | 7.299089     | 7.390287     |\n",
    "| HH-RF (Linear)       | 7.162677  | 7.116026     | 7.209328     |\n",
    "| HH-RF (Non Linear)   | 7.135319  | 7.089570     | 7.181067     |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cobra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
