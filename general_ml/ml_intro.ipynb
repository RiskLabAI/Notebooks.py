{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7ede91",
   "metadata": {},
   "source": [
    "# **Introduction to ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecf43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762ac13",
   "metadata": {},
   "source": [
    "# Supervised Learning: Simple Perceptron Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da8fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    ####\n",
    "    return 1 if x > 0 else -1\n",
    "\n",
    "def train_perceptron(X, y, learning_rate, epochs):\n",
    "    ####\n",
    "    N, D = X.shape\n",
    "    weights = np.zeros(D + 1)\n",
    "    X_biased = np.hstack((np.ones((N, 1)), X))\n",
    "    for epoch in range(epochs):\n",
    "        errors = 0\n",
    "        for i in range(N):\n",
    "            net_input = np.dot(X_biased[i], weights)\n",
    "            prediction = sign(net_input)\n",
    "            if prediction != y[i]:\n",
    "                errors += 1\n",
    "                weights = weights + learning_rate * y[i] * X_biased[i]\n",
    "        if errors == 0:\n",
    "            break\n",
    "    return weights\n",
    "\n",
    "def predict_perceptron(X_new, weights):\n",
    "    ####\n",
    "    X_new_biased = np.hstack((np.ones((X_new.shape[0], 1)), X_new))\n",
    "    predictions = []\n",
    "    for i in range(X_new_biased.shape[0]):\n",
    "        net_input = np.dot(X_new_biased[i], weights)\n",
    "        predictions.append(sign(net_input))\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Example Usage of Perceptron\n",
    "X_train_perceptron = np.array([\n",
    "    [2, 1],\n",
    "    [3, 2],\n",
    "    [1, 0.5],\n",
    "    [4, 3],\n",
    "    [1.5, 2]\n",
    "])\n",
    "y_train_perceptron = np.array([\n",
    "    -1,\n",
    "    1,\n",
    "    -1,\n",
    "    1,\n",
    "    -1\n",
    "])\n",
    "learning_rate_val = 0.1\n",
    "epochs_val = 10\n",
    "final_weights_perceptron = train_perceptron(X_train_perceptron, y_train_perceptron, learning_rate_val, epochs_val)\n",
    "X_new_data_perceptron = np.array([\n",
    "    [2.5, 1.5],\n",
    "    [0.5, 0.2],\n",
    "    [3.5, 2.8]\n",
    "])\n",
    "new_predictions_perceptron = predict_perceptron(X_new_data_perceptron, final_weights_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d5361",
   "metadata": {},
   "source": [
    "# Ensembles: Bagging (Random Forest Idea) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c181db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_bagging():\n",
    "    ####\n",
    "    np.random.seed(42)\n",
    "\n",
    "    X_ensemble = np.random.rand(100, 5)\n",
    "    y_ensemble = (X_ensemble[:, 0] + X_ensemble[:, 1] > 1.0).astype(int)\n",
    "\n",
    "    X_train_ensemble, X_test_ensemble, y_train_ensemble, y_test_ensemble = train_test_split(\n",
    "        X_ensemble, y_ensemble, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    single_tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "    single_tree.fit(X_train_ensemble, y_train_ensemble)\n",
    "    single_tree_pred = single_tree.predict(X_test_ensemble)\n",
    "    single_tree_accuracy = accuracy_score(y_test_ensemble, single_tree_pred)\n",
    "\n",
    "    bagging_model = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        n_estimators=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    bagging_model.fit(X_train_ensemble, y_train_ensemble)\n",
    "    bagging_pred = bagging_model.predict(X_test_ensemble)\n",
    "    bagging_accuracy = accuracy_score(y_test_ensemble, bagging_pred)\n",
    "\n",
    "\n",
    "demonstrate_bagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfb26e",
   "metadata": {},
   "source": [
    "# Deep Learning Architectures - Placeholder for complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9940cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conceptual_deep_neural_network():\n",
    "    ####\n",
    "    pass\n",
    "\n",
    "# Convolutional Neural Networks (CNNs) - Conceptual Layers\n",
    "def conceptual_cnn_layers():\n",
    "    ####\n",
    "    pass\n",
    "\n",
    "# Recurrent Neural Networks (RNNs) and Transformers - Conceptual\n",
    "def conceptual_rnn_transformer():\n",
    "    ####\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab184d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement Learning - Q-Learning Placeholder\n",
    "def conceptual_q_learning():\n",
    "    ####\n",
    "    pass\n",
    "\n",
    "# Generative Models: GANs - Conceptual Placeholder\n",
    "def conceptual_gan():\n",
    "    ####\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007e0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6\n",
    "# Ethical Considerations and Interpretability in ML: LIME Example\n",
    "def demonstrate_lime_interpretability():\n",
    "    ####\n",
    "    X_lime = np.array([\n",
    "        [10, 2, 8],\n",
    "        [5, 8, 3],\n",
    "        [9, 3, 7],\n",
    "        [6, 7, 4],\n",
    "        [11, 1, 9]\n",
    "    ])\n",
    "    y_lime = np.array([1, 0, 1, 0, 1])\n",
    "    feature_names = [\"ExamScore\", \"StudyHours\", \"SleepHours\"]\n",
    "    class_names = [\"Fail\", \"Pass\"]\n",
    "\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_lime, y_lime)\n",
    "\n",
    "    ####\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data=X_lime,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode='classification'\n",
    "    )\n",
    "\n",
    "    new_student_data = np.array([7, 5, 5])\n",
    "    \n",
    "    explanation = explainer.explain_instance(\n",
    "        data_row=new_student_data,\n",
    "        predict_fn=model.predict_proba,\n",
    "        num_features=len(feature_names),\n",
    "        top_labels=1\n",
    "    )\n",
    "\n",
    "\n",
    "demonstrate_lime_interpretability()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 6.1\n",
    "# Bias and Fairness in Algorithms - Conceptual Example of Data Mitigation\n",
    "def demonstrate_data_fairness_mitigation():\n",
    "    ####\n",
    "    data_biased = pd.DataFrame({\n",
    "        'Age': [30, 25, 35, 40, 28, 32, 29, 38],\n",
    "        'Income': [50000, 40000, 60000, 70000, 45000, 55000, 48000, 65000],\n",
    "        'Gender': ['M', 'F', 'M', 'M', 'F', 'F', 'M', 'F'],\n",
    "        'LoanApproved': [1, 0, 1, 1, 0, 1, 1, 0]\n",
    "    })\n",
    "\n",
    "    data_fairer = pd.DataFrame({\n",
    "        'Age': [30, 25, 35, 40, 28, 32, 29, 38],\n",
    "        'Income': [50000, 40000, 60000, 70000, 45000, 55000, 48000, 65000],\n",
    "        'Gender': ['M', 'F', 'M', 'M', 'F', 'F', 'M', 'F'],\n",
    "        'LoanApproved': [1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    })\n",
    "\n",
    "demonstrate_data_fairness_mitigation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e28b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning: Simple Ideas\n",
    "# Section 1.0 Machine Learning\n",
    "\n",
    "# This section talks about the general concept of Machine Learning.\n",
    "# No specific Python packages are directly mentioned here, as it's a high-level introduction.\n",
    "\n",
    "## Data-Driven Models\n",
    "# This subsection explains the difference between data-driven and model-driven approaches.\n",
    "# In Python, we often work with data using libraries like 'pandas' for data manipulation\n",
    "# and 'numpy' for numerical operations.\n",
    "\n",
    "import pandas as pd # For working with data tables\n",
    "import numpy as np  # For numerical operations, like arrays and calculations\n",
    "\n",
    "#### Example: Creating a simple dataset\n",
    "data = {\n",
    "    'feature1': [10, 20, 15, 25, 30],\n",
    "    'feature2': [1, 2, 1.5, 2.5, 3],\n",
    "    'label': ['cat', 'dog', 'cat', 'dog', 'cat']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# print(df) # To show the data frame, commented out for brevity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b236a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Feature Space\n",
    "# This subsection describes what features are and how they are represented.\n",
    "# Key packages: 'numpy' for vector operations, 'scipy.spatial.distance' for distance calculations.\n",
    "\n",
    "### Equation 2.1: Feature vector representation\n",
    "# In Python, a feature vector is simply a list or a NumPy array.\n",
    "feature_vector_example = np.array([1.0, 2.5, 3.0])\n",
    "# Here, d=3, and x_1j=1.0, x_2j=2.5, x_3j=3.0\n",
    "\n",
    "### Equation 2.2: Euclidean distance\n",
    "from scipy.spatial.distance import euclidean # Part of SciPy, a scientific computing library\n",
    "\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "euclidean_distance = euclidean(point1, point2)\n",
    "# print(f\"Euclidean Distance: {euclidean_distance}\") # Should be 5.0\n",
    "\n",
    "### Equation 2.3: Hamming distance\n",
    "# We can implement XOR for binary vectors.\n",
    "def hamming_distance(vec1, vec2):\n",
    "    return np.sum(vec1 != vec2)\n",
    "\n",
    "binary_vec1 = np.array([1, 0, 1])\n",
    "binary_vec2 = np.array([1, 1, 0])\n",
    "ham_dist = hamming_distance(binary_vec1, binary_vec2)\n",
    "# print(f\"Hamming Distance: {ham_dist}\") # Should be 2\n",
    "\n",
    "### Equation 2.4: Cosine similarity\n",
    "from numpy.linalg import norm # For calculating vector norms (lengths)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = norm(vec1)\n",
    "    norm_vec2 = norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 # Handle division by zero for zero vectors\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "vec_a = np.array([1, 1, 0])\n",
    "vec_b = np.array([1, 0, 1])\n",
    "cos_sim = cosine_similarity(vec_a, vec_b)\n",
    "# print(f\"Cosine Similarity: {cos_sim}\") # Example result\n",
    "\n",
    "### Equation 2.5: Joint probability for independent variables\n",
    "# This is a conceptual example in Python.\n",
    "# It shows how probabilities multiply for independent events.\n",
    "p_x1 = 0.5\n",
    "p_x2 = 0.4\n",
    "p_x3 = 0.3\n",
    "joint_probability = p_x1 * p_x2 * p_x3\n",
    "# print(f\"Joint Probability (independent): {joint_probability}\")\n",
    "\n",
    "#### Feature Selection and Feature Extraction\n",
    "# Libraries like 'sklearn.feature_selection' and 'sklearn.decomposition' are key here.\n",
    "from sklearn.feature_selection import SelectKBest, chi2 # For feature selection\n",
    "from sklearn.decomposition import PCA # For Principal Component Analysis (feature extraction)\n",
    "\n",
    "# Example for Feature Selection (conceptual)\n",
    "# X_data = np.array([[10, 0.5, 2], [12, 0.6, 1], [8, 0.4, 3], [15, 0.7, 0]])\n",
    "# y_labels = np.array([0, 1, 0, 1])\n",
    "# selector = SelectKBest(chi2, k=2)\n",
    "# X_new = selector.fit_transform(X_data, y_labels)\n",
    "# print(f\"Selected features:\\n{X_new}\")\n",
    "\n",
    "# Example for Feature Extraction (conceptual)\n",
    "# pca = PCA(n_components=2)\n",
    "# X_reduced = pca.fit_transform(X_data)\n",
    "# print(f\"PCA reduced features:\\n{X_reduced}\")\n",
    "\n",
    "## Supervised and Unsupervised Learning\n",
    "# This section distinguishes between supervised and unsupervised learning.\n",
    "# 'sklearn' (Scikit-learn) is the primary library for both.\n",
    "\n",
    "### Supervised Learning\n",
    "# 'sklearn.linear_model' for regression, 'sklearn.svm' for classification.\n",
    "from sklearn.model_selection import train_test_split # For splitting data\n",
    "from sklearn.linear_model import LogisticRegression # An example classifier\n",
    "\n",
    "# Example: Prepare data for supervised learning\n",
    "X = df[['feature1', 'feature2']] # Features\n",
    "y = df['label'] # Labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a supervised model\n",
    "model_supervised = LogisticRegression()\n",
    "model_supervised.fit(X_train, y_train)\n",
    "# print(f\"Supervised Model Score: {model_supervised.score(X_test, y_test)}\")\n",
    "\n",
    "### Unsupervised Learning\n",
    "# 'sklearn.cluster' for clustering, 'sklearn.manifold' for low-dimensional embedding.\n",
    "from sklearn.cluster import KMeans # An example clustering algorithm\n",
    "\n",
    "# Example: Prepare data for unsupervised learning (no labels needed for training)\n",
    "X_unsupervised = df[['feature1', 'feature2']]\n",
    "\n",
    "# Train an unsupervised model\n",
    "model_unsupervised = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "model_unsupervised.fit(X_unsupervised)\n",
    "# print(f\"Cluster Labels: {model_unsupervised.labels_}\")\n",
    "\n",
    "#### Active Learning Strategies (Conceptual in Python)\n",
    "# Active learning often involves a loop where the model makes predictions,\n",
    "# uncertainty is calculated, and then new data points are selected for labeling.\n",
    "# No direct 'sklearn' module for active learning, but tools can be combined.\n",
    "\n",
    "# from modAL.models import ActiveLearner # A dedicated library for active learning\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Example: Uncertainty sampling - selecting data with low confidence\n",
    "# # This often involves getting prediction probabilities from a model.\n",
    "# # For instance, if model_supervised is a classifier:\n",
    "# probabilities = model_supervised.predict_proba(X_test)\n",
    "# # You would then identify indices where max(probability) is close to 0.5 (for binary classification)\n",
    "\n",
    "## Generalization Performance\n",
    "# This section deals with error measurement and the concept of generalization.\n",
    "# 'sklearn.metrics' is crucial for loss functions and evaluating performance.\n",
    "\n",
    "### Equation 2.6: Squared Error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_true_reg = np.array([5, 10, 15])\n",
    "y_pred_reg = np.array([4, 11, 13])\n",
    "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "### Equation 2.7: Absolute Error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "# print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "### Equation 2.8: 0-1 Loss Function (Misclassification Error)\n",
    "# In sklearn, this is often the inverse of accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true_cls = np.array([0, 1, 0, 1])\n",
    "y_pred_cls = np.array([0, 0, 0, 1]) # One misclassification\n",
    "zero_one_loss = 1 - accuracy_score(y_true_cls, y_pred_cls)\n",
    "# print(f\"0-1 Loss: {zero_one_loss}\")\n",
    "\n",
    "### Equation 2.9: Hinge Loss\n",
    "# This is typically used internally by SVMs (Support Vector Machines).\n",
    "# You can implement it manually for conceptual understanding.\n",
    "def hinge_loss(y_true, y_pred_scores):\n",
    "    # y_true should be +1 or -1\n",
    "    # y_pred_scores are the decision function scores\n",
    "    return np.maximum(0, 1 - y_true * y_pred_scores)\n",
    "\n",
    "# Example:\n",
    "# y_true_hinge = np.array([1, -1, 1, -1])\n",
    "# y_pred_scores_hinge = np.array([0.8, -0.6, 0.1, -0.9])\n",
    "# h_loss = hinge_loss(y_true_hinge, y_pred_scores_hinge)\n",
    "# print(f\"Hinge Loss per sample: {h_loss}\")\n",
    "\n",
    "### Equation 2.10: Training Error (Empirical Risk)\n",
    "# This is the average of the loss function over the training data.\n",
    "training_error = np.mean(mean_squared_error(y_true_reg, y_pred_reg)) # Example using MSE\n",
    "# print(f\"Training Error (Empirical Risk): {training_error}\")\n",
    "\n",
    "### Equation 2.11: Example of Overfitting Function\n",
    "# This function is not typically implemented directly as a model in ML libraries,\n",
    "# but it illustrates a concept.\n",
    "\n",
    "### Equation 2.12 & 2.13: Test Error (Generalization Error)\n",
    "# This is calculated using unseen data, typically from X_test, y_test.\n",
    "test_predictions = model_supervised.predict(X_test)\n",
    "generalization_accuracy = accuracy_score(y_test, test_predictions)\n",
    "# print(f\"Generalization Accuracy: {generalization_accuracy}\")\n",
    "\n",
    "### Equation 2.14: Bias-Variance Decomposition (Conceptual)\n",
    "# This concept is critical but not a direct function in Python libraries.\n",
    "# It helps in understanding model trade-offs during development.\n",
    "\n",
    "## Model Complexity\n",
    "# This section discusses VC dimension and PAC learning.\n",
    "# No direct Python functions for VC dimension calculation in 'sklearn'.\n",
    "# It's more of a theoretical concept influencing model choices.\n",
    "\n",
    "### Equation 2.15: Vapnik's Theorem (Conceptual)\n",
    "# This is a theoretical bound. Libraries don't directly calculate this probability.\n",
    "# However, the idea of limiting model complexity is implemented through regularization.\n",
    "\n",
    "#### Regularization\n",
    "# 'sklearn' models often have 'C' (for SVMs) or 'alpha' (for Lasso/Ridge regression) parameters\n",
    "# to control regularization and thus model complexity.\n",
    "from sklearn.linear_model import Ridge # An example of a regularized model\n",
    "\n",
    "# ridge_model = Ridge(alpha=1.0) # alpha controls the amount of regularization\n",
    "# ridge_model.fit(X_train, y_train_numerical) # Assuming y_train is numerical for regression\n",
    "# print(f\"Ridge Model Coefficients: {ridge_model.coef_}\")\n",
    "\n",
    "## Ensembles\n",
    "# This section covers combining multiple models for better performance.\n",
    "# 'sklearn.ensemble' is the primary package.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Examples of ensemble methods\n",
    "from sklearn.tree import DecisionTreeClassifier # A common base classifier\n",
    "\n",
    "#### Bagging and Random Forests\n",
    "# RandomForestClassifier in 'sklearn' is an implementation of bagging with decision trees.\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # 100 trees\n",
    "rf_model.fit(X_train, y_train)\n",
    "# print(f\"Random Forest Score: {rf_model.score(X_test, y_test)}\")\n",
    "\n",
    "#### Stacking\n",
    "# 'sklearn.ensemble.StackingClassifier' (or Regressor) is available in newer sklearn versions.\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # Another base classifier\n",
    "\n",
    "# Example of Stacking (conceptual)\n",
    "# estimators = [\n",
    "#     ('lr', LogisticRegression(random_state=42)),\n",
    "#     ('rf', RandomForestClassifier(random_state=42))\n",
    "# ]\n",
    "# stacking_model = StackingClassifier(estimators=estimators, final_estimator=KNeighborsClassifier())\n",
    "# stacking_model.fit(X_train, y_train)\n",
    "# print(f\"Stacking Classifier Score: {stacking_model.score(X_test, y_test)}\")\n",
    "\n",
    "#### Boosting\n",
    "# 'sklearn.ensemble.GradientBoostingClassifier' is an example of boosting.\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "# print(f\"Gradient Boosting Score: {gb_model.score(X_test, y_test)}\")\n",
    "\n",
    "## Data Dependencies and Computational Complexity\n",
    "# This section discusses how data structure and dependencies affect computation time.\n",
    "# While no direct Python functions represent N:N or N:K dependencies as variables,\n",
    "# the choice of algorithm in 'sklearn' implicitly handles these complexities.\n",
    "\n",
    "#### Distance Matrix Calculation\n",
    "# Example of O(N^2 * d) complexity for a distance matrix.\n",
    "# If you have N points with d features:\n",
    "num_points = 100\n",
    "num_features = 10\n",
    "random_data = np.random.rand(num_points, num_features)\n",
    "\n",
    "# dist_matrix = euclidean_distances(random_data) # This would calculate all N*N distances\n",
    "# from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "#### Tree-based Spatial Index (Conceptual)\n",
    "# Libraries like 'scipy.spatial.KDTree' or 'sklearn.neighbors.NearestNeighbors'\n",
    "# use tree-like structures to speed up nearest neighbor searches, reducing complexity.\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# nn_model = NearestNeighbors(n_neighbors=5, algorithm='kd_tree')\n",
    "# nn_model.fit(random_data)\n",
    "# distances, indices = nn_model.kneighbors(random_data)\n",
    "# print(f\"Distances to 5 nearest neighbors:\\n{distances[:5]}\")\n",
    "\n",
    "#### Neural Networks and N:K Dependency\n",
    "# 'tensorflow' or 'pytorch' are popular libraries for building neural networks.\n",
    "# Their architectures inherently manage the N:K dependency through weights.\n",
    "# The number of weights K is usually much smaller than the number of data points N.\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# model_nn = keras.Sequential([\n",
    "#     keras.layers.Dense(units=10, activation='relu', input_shape=(num_features,)), # K=10 weights (plus biases)\n",
    "#     keras.layers.Dense(units=1, activation='sigmoid')\n",
    "# ])\n",
    "# model_nn.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# model_nn.fit(random_data, np.random.randint(0, 2, num_points), epochs=10)\n",
    "# This example is conceptual to show how NN layers have a set number of units/weights (K)\n",
    "# independent of the number of training samples (N) once the architecture is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c9c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risklab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
