{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import necessary modules \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "from numpy.linalg import eigvalsh\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rbf_volatility_surface import RBFVolatilitySurface\n",
    "from smoothness_prior import RBFQuadraticSmoothnessPrior\n",
    "from dataset_sabr import generate_sabr_call_options\n",
    "from surface_vae_trainer import SurfaceVAETrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the strike price list and maturity time list\n",
    "strike_price_list = np.array([0.75, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.2, 1.3, 1.5])\n",
    "maturity_time_list = np.array([0.02, 0.08, 0.17, 0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0])\n",
    "\n",
    "# Create the product grid of maturity times and strike prices\n",
    "product_grid = list(product(maturity_time_list, strike_price_list))\n",
    "maturity_times, strike_prices = zip(*product_grid)\n",
    "\n",
    "# Convert to arrays for further operations\n",
    "maturity_times = np.array(maturity_times)\n",
    "strike_prices = np.array(strike_prices)\n",
    "\n",
    "# Variance formula for log-uniform distribution\n",
    "def log_uniform_variance(a, b):\n",
    "    log_term = np.log(b / a)\n",
    "    var = ((b ** 2 - a ** 2) / (2 * log_term)) - ((b - a) / log_term) ** 2\n",
    "    return var\n",
    "\n",
    "# Calculate standard deviations for maturity times and strike prices\n",
    "maturity_std = np.sqrt(log_uniform_variance(maturity_time_list.min(), maturity_time_list.max()))\n",
    "strike_std = np.sqrt(log_uniform_variance(strike_price_list.min(), strike_price_list.max()))\n",
    "\n",
    "# Define the SABR model parameters\n",
    "alpha = 0.20  # Stochastic volatility parameter\n",
    "beta = 0.50   # Elasticity parameter\n",
    "rho = -0.75   # Correlation between asset price and volatility\n",
    "nu = 1.0      # Volatility of volatility parameter\n",
    "\n",
    "# Other model parameters\n",
    "risk_free_rate = np.log(1.02)  # Risk-free interest rate\n",
    "underlying_price = 1.0         # Current price of the underlying asset\n",
    "\n",
    "# Generate the dataset using the SABR model and Black-Scholes formula\n",
    "call_option_dataset = generate_sabr_call_options(\n",
    "    alpha=alpha,\n",
    "    beta=beta,\n",
    "    rho=rho,\n",
    "    nu=nu,\n",
    "    maturity_times=maturity_times,\n",
    "    strike_prices=strike_prices,\n",
    "    risk_free_rate=risk_free_rate,\n",
    "    underlying_price=underlying_price\n",
    ")\n",
    "\n",
    "# Maturity times and strike prices from the previous product grid setup\n",
    "hypothetical_maturity_time_list = np.logspace(np.log10(0.01), np.log10(3.1), 100)\n",
    "hypothetical_strike_price_list = np.logspace(np.log10(0.7), np.log10(1.75), 100)\n",
    "\n",
    "# Create the product grid of maturity times and strike prices\n",
    "hypothetical_product_grid = list(product(hypothetical_maturity_time_list, hypothetical_strike_price_list))\n",
    "hypothetical_maturity_times, hypothetical_strike_prices = zip(*hypothetical_product_grid)\n",
    "hypothetical_maturity_times, hypothetical_strike_prices = np.array(hypothetical_maturity_times), np.array(hypothetical_strike_prices)\n",
    "\n",
    "# Reshape the data for 3D surface plotting\n",
    "hypothetical_maturities_grid = hypothetical_maturity_times.reshape((len(hypothetical_maturity_time_list), len(hypothetical_strike_price_list)))  \n",
    "hypothetical_strikes_grid = hypothetical_strike_prices.reshape((len(hypothetical_maturity_time_list), len(hypothetical_strike_price_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_roots = 350\n",
    "# n_roots = 10\n",
    "smoothness_controller = 3.274549162877732e-05\n",
    "\n",
    "# Initialize the RBFQuadraticSmoothnessPrior class\n",
    "smoothness_prior = RBFQuadraticSmoothnessPrior(\n",
    "    maturity_times=maturity_times,\n",
    "    strike_prices=strike_prices,\n",
    "    maturity_std=maturity_std,\n",
    "    strike_std=strike_std,\n",
    "    n_roots=n_roots,\n",
    "    smoothness_controller=smoothness_controller,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "prior_covariance_matrix = smoothness_prior.prior_covariance()\n",
    "prior_eigenvalues = np.sort(np.linalg.eigvalsh(prior_covariance_matrix))[::-1].copy()\n",
    "\n",
    "# The constant_volatility is set to a reasonable value\n",
    "constant_volatility = RBFVolatilitySurface.calculate_constant_volatility(\n",
    "    call_option_dataset[\"Implied Volatility\"],\n",
    "    call_option_dataset[\"Time to Maturity\"],\n",
    "    call_option_dataset[\"Strike Price\"],\n",
    "    risk_free_rate,\n",
    "    underlying_price\n",
    ")\n",
    "\n",
    "sampled_surface_coefficients = smoothness_prior.sample_smooth_surfaces(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the sampled coefficients \n",
    "sampled_volatilities = []\n",
    "for coefficients in sampled_surface_coefficients:\n",
    "    \n",
    "    # Initialize the RBFVolatilitySurface class for each set of coefficients\n",
    "    rbf_surface = RBFVolatilitySurface(\n",
    "        coefficients=coefficients,\n",
    "        maturity_times=maturity_times,\n",
    "        strike_prices=strike_prices,\n",
    "        maturity_std=maturity_std,\n",
    "        strike_std=strike_std,\n",
    "        constant_volatility=constant_volatility\n",
    "    )\n",
    "\n",
    "    # Generate the volatility surface over the product grid of times and strikes\n",
    "    surface_volatilities = [\n",
    "        rbf_surface.implied_volatility_surface(T, K)\n",
    "        for T, K in product_grid\n",
    "    ]\n",
    "    sampled_volatilities.extend(surface_volatilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 70  # Latent dimension\n",
    "data_dim = 100  # Data dimension of input\n",
    "latent_diagonal = prior_eigenvalues[:latent_dim]  # Eigenvalues for latent prior\n",
    "batch_size = 1000  # Batch size for training\n",
    "beta_ = 1.0  # Beta value for beta-VAE\n",
    "fine_tune_learning_rate = 1e-4  # Fine-tune learning rate\n",
    "pre_train_epochs = 350  # Number of pre-train epochs\n",
    "fine_tune_epochs = 20  # Number of fine-tune epochs\n",
    "device = \"cpu\"  # Use CPU as the device\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "hidden_dim_grid = [128, 256, 512]  # Example grid for hidden_dim\n",
    "n_layers_grid = [2, 4, 8]         # Example grid for n_layers\n",
    "pre_train_learning_rate_grid = [1e-4, 1e-3, 1e-2]  # Example grid for learning rate\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Define the grid search\n",
    "grid = itertools.product(hidden_dim_grid, n_layers_grid, pre_train_learning_rate_grid)\n",
    "\n",
    "for hidden_dim, n_layers, pre_train_learning_rate in tqdm(grid):\n",
    "    # Initialize the trainer with the specified configuration\n",
    "    trainer = SurfaceVAETrainer(\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        n_layers=n_layers,\n",
    "        data_dim=data_dim,\n",
    "        latent_diagonal=latent_diagonal,\n",
    "        batch_size=batch_size,\n",
    "        beta=beta_,\n",
    "        pre_train_learning_rate=pre_train_learning_rate,\n",
    "        fine_tune_learning_rate=fine_tune_learning_rate,\n",
    "        pre_train_epochs=pre_train_epochs,\n",
    "        fine_tune_epochs=fine_tune_epochs,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Train the model using pre_train\n",
    "    trainer.pre_train_with_sampling(\n",
    "        smoothness_prior=smoothness_prior,\n",
    "        experiment_name=f\"test_hd_{hidden_dim}_nl_{n_layers}_lr_{pre_train_learning_rate}\"\n",
    "    )\n",
    "\n",
    "    # Retrieve the last row of the loss history (assuming it's stored in trainer.pre_train_loss_history)\n",
    "    loss_df = pd.DataFrame(trainer.pre_train_loss_history)\n",
    "    last_row = loss_df.iloc[-1].copy()\n",
    "\n",
    "    # Add the configuration as columns in the last row\n",
    "    last_row['hidden_dim'] = hidden_dim\n",
    "    last_row['n_layers'] = n_layers\n",
    "    last_row['pre_train_learning_rate'] = pre_train_learning_rate\n",
    "\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([last_row])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank the losses for each column (except 'Total Loss')\n",
    "ranked_losses = results_df.drop(columns=['Total Loss', 'hidden_dim', 'n_layers', 'pre_train_learning_rate']).rank()\n",
    "\n",
    "ranked_df = results_df.copy()\n",
    "\n",
    "# Compute the average rank for each configuration\n",
    "ranked_df['average_rank'] = ranked_losses.mean(axis=1)\n",
    "\n",
    "# Sort by the average rank (lower is better)\n",
    "ranked_df = ranked_df.sort_values('average_rank')\n",
    "\n",
    "# Print the top-ranked configurations\n",
    "ranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 70  # Latent dimension\n",
    "data_dim = 100  # Data dimension of input\n",
    "hidden_dim = 512\n",
    "n_layers = 8\n",
    "latent_diagonal = prior_eigenvalues[:latent_dim]  # Eigenvalues for latent prior\n",
    "batch_size = 1000  # Batch size for training\n",
    "beta_ = 1.0  # Beta value for beta-VAE\n",
    "pre_train_learning_rate = 1e-3\n",
    "fine_tune_learning_rate = 1e-4  # Fine-tune learning rate\n",
    "pre_train_epochs = 600  # Number of pre-train epochs\n",
    "fine_tune_epochs = 20  # Number of fine-tune epochs\n",
    "device = \"cpu\"  # Use CPU as the device\n",
    "\n",
    "trainer = SurfaceVAETrainer(\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    "    data_dim=data_dim,\n",
    "    latent_diagonal=latent_diagonal,\n",
    "    batch_size=batch_size,\n",
    "    beta=beta_,\n",
    "    pre_train_learning_rate=pre_train_learning_rate,\n",
    "    fine_tune_learning_rate=fine_tune_learning_rate,\n",
    "    pre_train_epochs=pre_train_epochs,\n",
    "    fine_tune_epochs=fine_tune_epochs,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Train the model using pre_train\n",
    "trainer.pre_train_with_sampling(\n",
    "    smoothness_prior=smoothness_prior,\n",
    "    experiment_name=\"test vae\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = pd.DataFrame(trainer.pre_train_loss_history)\n",
    "\n",
    "# Create a subplot figure with 1x2 grid for individual losses, and a second row spanning the entire width for total loss\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\"Reconstruction Loss\", \"KL Loss\", \"Total Loss\"),\n",
    "    specs=[[{'type': 'scatter'}, {'type': 'scatter'}],\n",
    "           [{'colspan': 2, 'type': 'scatter'}, None]],\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Add traces for individual losses\n",
    "fig.add_trace(go.Scatter(x=loss_history.index, y=loss_history[\"Reconstruction Loss\"], mode=\"lines\", name=\"Reconstruction Loss\"), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=loss_history.index, y=loss_history[\"KL Loss\"], mode=\"lines\", name=\"KL Loss\"), row=1, col=2)\n",
    "\n",
    "# Add a trace for the total loss spanning the entire second row\n",
    "fig.add_trace(go.Scatter(x=loss_history.index, y=loss_history[\"Total Loss\"], mode=\"lines\", name=\"Total Loss\"), row=2, col=1)\n",
    "\n",
    "# Update the layout to include 'Iterations' as the x-axis name for each subplot\n",
    "fig.update_xaxes(title_text=\"Iterations\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Iterations\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"Iterations\", row=2, col=1)  # The third row spans two columns\n",
    "\n",
    "fig.update_yaxes(type=\"log\", row=1, col=1)\n",
    "fig.update_yaxes(type=\"log\", row=1, col=2)\n",
    "fig.update_yaxes(type=\"log\", row=2, col=1)  # The third row spans two columns\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(height=900, width=900, title_text=\"Beta-VAE Training Losses\", showlegend=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
