{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐ Tutorial: Feature Importance with RiskLabAI\n",
    "\n",
    "This notebook is a tutorial for the feature importance methods in the `RiskLabAI` library, based on Chapters 6-8 of 'Advances in Financial Machine Learning' by Marcos López de Prado.\n",
    "\n",
    "We will demonstrate the professional **Strategy Pattern** design of this module (using `FeatureImportanceController`) to test five different methods on a unified synthetic dataset.\n",
    "\n",
    "**Outline:**\n",
    "1.  **Generate Synthetic Data:** Create a dataset with known informative, redundant, and noise features using `get_test_dataset`.\n",
    "2.  **Part 1: Standard Feature Importance:**\n",
    "    * Mean Decrease Impurity (MDI)\n",
    "    * Mean Decrease Accuracy (MDA)\n",
    "    * Single Feature Importance (SFI)\n",
    "3.  **Part 2: Clustered Feature Importance:**\n",
    "    * First, we'll use `cluster_k_means_top` to group our features.\n",
    "    * Clustered MDI\n",
    "    * Clustered MDA\n",
    "4.  **Part 3: Orthogonalized Importance:**\n",
    "    * Use `orthogonal_features` to remove collinearity.\n",
    "    * Calculate MDI on the orthogonal features.\n",
    "    * Use `calculate_weighted_tau` to compare results.\n",
    "5.  **Conclusion:** Compare the results and see which methods correctly identified the informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports\n",
    "\n",
    "First, we import our libraries. We'll import `pandas`, `numpy`, `matplotlib`, and `sklearn`. \n",
    "\n",
    "From `RiskLabAI`, we import our `feature_importance` module (which contains the `FeatureImportanceController` and all utilities) and our plotting utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# RiskLabAI Imports\n",
    "import RiskLabAI.features.feature_importance as fi\n",
    "import RiskLabAI.utils.publication_plots as pub_plots\n",
    "\n",
    "# --- Notebook Configuration ---\n",
    "warnings.filterwarnings('ignore')\n",
    "pub_plots.setup_publication_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We use `get_test_dataset` to create our ground truth. We will generate a dataset with 40 features:\n",
    "* **10 Informative (`I_`):** These features actually predict the target.\n",
    "* **15 Redundant (`R_`):** These are copies of the informative features, plus some noise. Models can easily overfit to these.\n",
    "* **15 Noise (`N_`):** These have no predictive value.\n",
    "\n",
    "**The goal of a good feature importance algorithm is to assign high importance only to the 10 Informative features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fi.get_test_dataset(\n",
    "    n_features=40,\n",
    "    n_informative=10,\n",
    "    n_redundant=15,\n",
    "    n_samples=10000,\n",
    "    random_state=42,\n",
    "    sigma_std=0.5 # Add 50% noise to redundant features\n",
    ")\n",
    "\n",
    "print(\"Features (X):\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget (y):\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part 1: Standard Feature Importance\n",
    "\n",
    "We will now test the three standard (non-clustered) methods. We will use a basic `RandomForestClassifier` as our model for all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot importance\n",
    "def plot_importance(importance_df, title):\n",
    "    importance_df.sort_values('Mean', ascending=True, inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(12, 16))\n",
    "    ax.barh(\n",
    "        importance_df.index,\n",
    "        importance_df['Mean'],\n",
    "        xerr=importance_df['StandardDeviation'],\n",
    "        color='C0'\n",
    "    )\n",
    "    pub_plots.apply_plot_style(ax, title, 'Feature Importance', 'Feature Name')\n",
    "    plt.show()\n",
    "\n",
    "# Define the base classifier for all strategies\n",
    "base_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    criterion='entropy', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mean Decrease Impurity (MDI)\n",
    "\n",
    "MDI is the *fastest* method. It is calculated in-sample (on training data) and measures how much each feature decreases impurity (Gini/Entropy). \n",
    "\n",
    "**Problem:** It is known to be biased and will inflate the importance of redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running MDI...\")\n",
    "# 1. Initialize the controller with the 'MDI' strategy\n",
    "mdi_controller = fi.FeatureImportanceController(\n",
    "    strategy_type='MDI', \n",
    "    classifier=base_classifier\n",
    ")\n",
    "\n",
    "# 2. Compute importance\n",
    "mdi_importance = mdi_controller.calculate_importance(X, y)\n",
    "\n",
    "plot_importance(mdi_importance, 'Feature Importance (MDI)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** MDI fails completely. As predicted, it assigns high importance to *both* the Informative (`I_`) and Redundant (`R_`) features, and incorrectly assigns zero importance to some Informative features. **It cannot distinguish signal from noise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mean Decrease Accuracy (MDA)\n",
    "\n",
    "MDA is a more robust, out-of-sample method. It works by:\n",
    "1. Training a model in a cross-validation loop.\n",
    "2. Measuring the baseline score (e.g., log-loss) on the test set.\n",
    "3. Shuffling *one feature* in the test set and measuring the new, worse score.\n",
    "4. The drop in score is the feature's importance.\n",
    "\n",
    "**This is much slower, but far more accurate than MDI.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running MDA (this may take a minute)...\")\n",
    "# 1. Initialize the controller\n",
    "mda_controller = fi.FeatureImportanceController(\n",
    "    strategy_type='MDA',\n",
    "    classifier=base_classifier,\n",
    "    n_splits=5 # 5-fold CV\n",
    ")\n",
    "\n",
    "# 2. Compute importance\n",
    "mda_importance = mda_controller.calculate_importance(X, y)\n",
    "\n",
    "plot_importance(mda_importance, 'Feature Importance (MDA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** MDA is a massive improvement. It correctly assigns high importance to all 10 Informative (`I_`) features while correctly identifying that the Redundant (`R_`) and Noise (`N_`) features have zero importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Single Feature Importance (SFI)\n",
    "\n",
    "SFI measures the predictive power of each feature *in isolation*. It trains a model on *only* that one feature and measures its cross-validated score.\n",
    "\n",
    "**Problem:** It fails to identify features that are only important in combination with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running SFI (this may take a minute)...\")\n",
    "# 1. Initialize the controller\n",
    "sfi_controller = fi.FeatureImportanceController(\n",
    "    strategy_type='SFI',\n",
    "    classifier=base_classifier,\n",
    "    n_splits=5,\n",
    "    scoring='log_loss'\n",
    ")\n",
    "\n",
    "# 2. Compute importance\n",
    "sfi_importance = sfi_controller.calculate_importance(X, y)\n",
    "\n",
    "plot_importance(sfi_importance, 'Feature Importance (SFI)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** SFI works, but it incorrectly gives high importance to Redundant features. This is because, in isolation, a redundant feature is just as predictive as its informative original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part 2: Clustered Feature Importance\n",
    "\n",
    "The problem with MDA is that when features are highly correlated (like our `I_` and `R_` features), shuffling one has little effect because the redundant ones provide cover. \n",
    "\n",
    "**Clustered Importance** solves this. It first groups features into clusters, and then shuffles the *entire cluster* at once.\n",
    "\n",
    "First, we use `cluster_k_means_top` (from the `RiskLabAI.cluster` module, imported via our `__init__.py`) to find the true clusters in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finding feature clusters...\")\n",
    "corr = X.corr()\n",
    "corr_sorted, clusters, silh = fi.cluster_k_means_top(\n",
    "    corr,\n",
    "    max_clusters=int(X.shape[1]/2), # Max 20 clusters\n",
    "    iterations=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Found {len(clusters)} clusters.\")\n",
    "\n",
    "# Plot the sorted correlation matrix to confirm clusters\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_sorted, ax=ax, cmap='viridis', vmin=-1, vmax=1)\n",
    "pub_plots.apply_plot_style(ax, 'Clustered Correlation Matrix', '', '')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Clustered MDI\n",
    "\n",
    "This method first calculates standard MDI (fast, but biased) and then sums the importances for each cluster. This is an improvement, but still based on a flawed MDI foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Clustered MDI...\")\n",
    "# 1. Initialize the controller\n",
    "c_mdi_controller = fi.FeatureImportanceController(\n",
    "    strategy_type='ClusteredMDI',\n",
    "    classifier=base_classifier,\n",
    "    clusters=clusters # Pass in our found clusters\n",
    ")\n",
    "\n",
    "# 2. Compute importance\n",
    "c_mdi_importance = c_mdi_controller.calculate_importance(X, y)\n",
    "\n",
    "plot_importance(c_mdi_importance, 'Clustered Feature Importance (MDI)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Clustered MDA\n",
    "\n",
    "This is the most robust method. It's like MDA, but instead of shuffling one feature at a time, it shuffles all features in a cluster. This defeats the substitution effect from redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Clustered MDA (this may take a minute)...\")\n",
    "# 1. Initialize the controller\n",
    "c_mda_controller = fi.FeatureImportanceController(\n",
    "    strategy_type='ClusteredMDA',\n",
    "    classifier=base_classifier,\n",
    "    clusters=clusters, # Pass in our found clusters\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "# 2. Compute importance\n",
    "c_mda_importance = c_mda_controller.calculate_importance(X, y)\n",
    "\n",
    "plot_importance(c_mda_importance, 'Clustered Feature Importance (MDA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** Clustered MDA gives the cleanest result. The chart clearly shows a small number of clusters (which contain the Informative features) are important, while all other clusters are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 3: Orthogonal Features & Weighted Tau\n",
    "\n",
    "An alternative to clustering is **orthogonalization**. This method uses PCA to transform the features into a set of uncorrelated components. We can then run MDI (which is fast) on these new features. The importance of the *first* principal components should be highest.\n",
    "\n",
    "We use `weighted_tau` to measure if the most important features (by MDI) align with the most important principal components (by explained variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Orthogonal Feature Importance...\")\n",
    "# 1. Get orthogonal features (X_ortho) and their component weights (pca_df)\n",
    "X_ortho, pca_df = fi.orthogonal_features(X, variance_threshold=0.95)\n",
    "\n",
    "# 2. Run MDI on the *orthogonal features*\n",
    "ortho_mdi_controller = fi.FeatureImportanceController(\n",
    "    strategy_type='MDI', \n",
    "    classifier=base_classifier\n",
    ")\n",
    "ortho_importance = ortho_mdi_controller.calculate_importance(X_ortho, y)\n",
    "\n",
    "# 3. Calculate Weighted Tau\n",
    "# This checks if feature importance (MDI) correlates with PC rank.\n",
    "mdi_ranks = ortho_importance['Mean'].rank(ascending=False)\n",
    "pca_ranks = pd.Series(range(1, len(mdi_ranks) + 1), index=mdi_ranks.index)\n",
    "\n",
    "tau = fi.calculate_weighted_tau(mdi_ranks, pca_ranks)\n",
    "\n",
    "print(f\"\\nWeighted Kendall's Tau: {tau:.4f}\")\n",
    "plot_importance(ortho_importance, 'Feature Importance on Orthogonal Features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** The plot shows that importance is concentrated in the first few Principal Components, which is what we expect. The high Weighted Tau score confirms that the feature importance ranking (from MDI) strongly correlates with the PCA component ranking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
