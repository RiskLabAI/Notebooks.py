{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⭐ Tutorial: Entropy Features with RiskLabAI\n",
    "\n",
    "This notebook is a tutorial for the various entropy estimators in the `RiskLabAI` library.\n",
    "\n",
    "**Entropy** is a measure of unpredictability, randomness, or complexity. In finance, it can be used to:\n",
    "* Measure market efficiency (higher entropy = more random/efficient).\n",
    "* Detect changes in market regimes.\n",
    "* Serve as a feature for machine learning models.\n",
    "\n",
    "We will demonstrate:\n",
    "1.  **Data Preparation:** Convert a continuous price series into a discrete string \"message\", which is the required input for entropy estimators.\n",
    "2.  **Benchmark Messages:** Create a perfectly random string and a perfectly ordered (predictable) string to use as benchmarks.\n",
    "3.  **Shannon Entropy:** Apply the classic `shannon_entropy` function.\n",
    "4.  **Plug-In Estimator:** Use `plug_in_entropy_estimator` to analyze entropy of n-grams (sequences).\n",
    "5.  **Lempel-Ziv (LZ) Entropy:** Apply `lempel_ziv_entropy`, a complexity-based estimator.\n",
    "6.  **Kontoyiannis Estimator:** Apply the more advanced `kontoyiannis_entropy` estimator with both expanding and rolling windows.\n",
    "7.  **Conclusion:** Compare the results from all estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports\n",
    "\n",
    "First, we import our libraries. We'll use `yfinance` to download sample stock data and our `RiskLabAI` modules for entropy and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# RiskLabAI Imports\n",
    "import RiskLabAI.features.entropy_features as ent\n",
    "import RiskLabAI.utils.publication_plots as pub_plots\n",
    "\n",
    "# --- Notebook Configuration ---\n",
    "np.random.seed(42)\n",
    "pub_plots.setup_publication_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation: Discretizing a Price Series\n",
    "\n",
    "Entropy estimators do not work on continuous price data. They require a discrete \"message\" (a string). A common way to create this is to:\n",
    "1.  Calculate returns.\n",
    "2.  Discretize the returns into a few symbols. Here, we'll use `1` for a positive return and `0` for a negative return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load SPY data\n",
    "spy = yf.Ticker(\"SPY\").history(start=\"2010-01-01\", end=\"2023-01-01\")['Close']\n",
    "returns = spy.pct_change().dropna()\n",
    "\n",
    "# 2. Discretize returns into a binary string\n",
    "spy_message = \"\".join(returns.apply(lambda x: '1' if x > 0 else '0'))\n",
    "\n",
    "print(f\"SPY Message (first 100 chars):\\n{spy_message[:100]}...\")\n",
    "print(f\"\\nTotal length: {len(spy_message)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Messages\n",
    "\n",
    "To understand the entropy values, we need benchmarks. We'll create two more messages of the same length as our SPY data:\n",
    "* **Ordered Message:** A perfectly predictable string of all `1`s. This should have an entropy of **0**.\n",
    "* **Random Message:** A perfectly random binary string. This should have the maximum possible entropy (which is `log2(2) = 1` for a binary alphabet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_len = len(spy_message)\n",
    "\n",
    "# 1. Ordered (predictable) message\n",
    "ordered_message = \"1\" * msg_len\n",
    "\n",
    "# 2. Random (unpredictable) message\n",
    "random_message = \"\".join(np.random.choice(['0', '1'], msg_len))\n",
    "\n",
    "print(f\"Ordered Message: {ordered_message[:100]}...\")\n",
    "print(f\"Random Message:  {random_message[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shannon Entropy (Snippet 2.1)\n",
    "\n",
    "Shannon Entropy is the classic measure. It calculates the average level of \"information\" or \"surprise\" from the probability of observing each *individual character*.\n",
    "\n",
    "* A string of all `1`s has `P(1)=1` and `P(0)=0`. Entropy = `-1*log2(1) = 0`.\n",
    "* A random string has `P(1)=0.5` and `P(0)=0.5`. Entropy = `-(0.5*log2(0.5) + 0.5*log2(0.5)) = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_spy_shannon = ent.shannon_entropy(spy_message)\n",
    "h_ord_shannon = ent.shannon_entropy(ordered_message)\n",
    "h_rand_shannon = ent.shannon_entropy(random_message)\n",
    "\n",
    "print(f\"Shannon Entropy (SPY):     {h_spy_shannon:.4f}\")\n",
    "print(f\"Shannon Entropy (Random):  {h_rand_shannon:.4f}\")\n",
t    "print(f\"Shannon Entropy (Ordered): {h_ord_shannon:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** Shannon entropy correctly identifies the random string as having maximum entropy (1.0) and the ordered string as having zero entropy (0.0). The SPY message is very close to 1.0, suggesting that on a *daily* basis, the direction of the market is nearly random (i.e., `P(up) ≈ P(down)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plug-In Estimator (Snippet 2.2)\n",
    "\n",
    "Shannon entropy only looks at individual characters. The Plug-In estimator is a more advanced version that looks at **sequences of characters (n-grams)**. This helps capture short-term patterns (serial correlation).\n",
    "\n",
    "We'll test it with `approximate_word_length=2` (e.g., \"01\", \"11\", \"10\", \"00\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_spy_plugin = ent.plug_in_entropy_estimator(spy_message, approximate_word_length=2)\n",
    "h_ord_plugin = ent.plug_in_entropy_estimator(ordered_message, approximate_word_length=2)\n",
    "h_rand_plugin = ent.plug_in_entropy_estimator(random_message, approximate_word_length=2)\n",
    "\n",
    "print(f\"Plug-In (n=2) Entropy (SPY):     {h_spy_plugin:.4f}\")\n",
    "print(f\"Plug-In (n=2) Entropy (Random):  {h_rand_plugin:.4f}\")\n",
    "print(f\"Plug-In (n=2) Entropy (Ordered): {h_ord_plugin:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** The results are very similar to Shannon entropy, as expected. The SPY message (0.9995) is slightly less random than the pure random string (0.9998), suggesting some very weak 2-day patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lempel-Ziv (LZ) Entropy (Snippet 2.3)\n",
    "\n",
    "LZ entropy is a measure of **complexity**. It works by parsing the string and counting how many *new, unique substrings* it finds. \n",
    "\n",
    "* A random string will have many new substrings, so entropy will be high.\n",
    "* An ordered string (`111...`) will only have *one* substring (`1`), so entropy will be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_spy_lz = ent.lempel_ziv_entropy(spy_message)\n",
    "h_ord_lz = ent.lempel_ziv_entropy(ordered_message)\n",
    "h_rand_lz = ent.lempel_ziv_entropy(random_message)\n",
    "\n",
    "print(f\"Lempel-Ziv Entropy (SPY):     {h_spy_lz:.4f}\")\n",
    "print(f\"Lempel-Ziv Entropy (Random):  {h_rand_lz:.4f}\")\n",
    "print(f\"Lempel-Ziv Entropy (Ordered): {h_ord_lz:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** The results are very different in scale, as LZ is a measure of complexity, not probability. However, the relative ordering is the same: Random (0.21) > SPY (0.19) > Ordered (0.0003). This confirms the SPY message is complex, but slightly *less* complex than pure noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kontoyiannis Estimator (Snippet 2.4/2.5)\n",
    "\n",
    "This is another LZ-based estimator. It measures entropy by finding the `longest_match_length` for a substring in the data it has already seen (the look-back window).\n",
    "\n",
    "We can run it in two modes:\n",
    "* **Expanding Window (`window=None`):** Looks back at all preceding data. Better for stationary series.\n",
    "* **Rolling Window (`window=...`):** Looks back a fixed number of steps. Better for non-stationary series (like markets) as it adapts to new patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding Window\n",
    "h_spy_k_exp = ent.kontoyiannis_entropy(spy_message, window=None)\n",
    "h_rand_k_exp = ent.kontoyiannis_entropy(random_message, window=None)\n",
    "\n",
    "# Rolling Window (e.g., 1 year = 252 days)\n",
    "h_spy_k_roll = ent.kontoyiannis_entropy(spy_message, window=252)\n",
    "h_rand_k_roll = ent.kontoyiannis_entropy(random_message, window=252)\n",
    "\n",
    "print(\"--- Expanding Window ---\")\n",
    "print(f\"Kontoyiannis (SPY):     {h_spy_k_exp:.4f}\")\n",
    "print(f\"Kontoyiannis (Random):  {h_rand_k_exp:.4f}\")\n",
    "\n",
    "print(\"--- Rolling Window (252) ---\")\n",
    "print(f\"Kontoyiannis (SPY):     {h_spy_k_roll:.4f}\")\n",
    "print(f\"Kontoyiannis (Random):  {h_rand_k_roll:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis:** This estimator gives a value in bits, similar to Shannon. Again, the SPY message (~0.99) is slightly more predictable (lower entropy) than the pure random message (~1.00)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "We have successfully applied four different entropy estimators to a discretized financial time series. Let's compare the results (scaling LZ for visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'SPY': {\n",
    "        'Shannon': h_spy_shannon,\n",
    "        'Plug-In (n=2)': h_spy_plugin,\n",
    "        'Kontoyiannis (Roll)': h_spy_k_roll,\n",
    "        'Lempel-Ziv (scaled)': h_spy_lz / h_rand_lz # Scale for comparison\n",
    "    },\n",
    "    'Random': {\n",
    "        'Shannon': h_rand_shannon,\n",
    "        'Plug-In (n=2)': h_rand_plugin,\n",
    "        'Kontoyiannis (Roll)': h_rand_k_roll,\n",
    "        'Lempel-Ziv (scaled)': 1.0\n",
    "    },\n",
    "    'Ordered': {\n",
    "        'Shannon': h_ord_shannon,\n",
    "        'Plug-In (n=2)': h_ord_plugin,\n",
    "        'Kontoyiannis (Roll)': 0.0, # Not enough data for rolling window on '111...'\n",
    "        'Lempel-Ziv (scaled)': h_ord_lz / h_rand_lz\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results).T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "df.plot(kind='bar', ax=ax, width=0.8)\n",
    "pub_plots.apply_plot_style(\n",
    "    ax, \n",
    "    'Entropy Estimator Comparison (Normalized)', \n",
    "    'Message Type', \n",
    "    'Entropy Value'\n",
    ")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Result:** All estimators agree on the relative order of randomness:\n",
    "\n",
    "**Random > SPY > Ordered**\n",
    "\n",
    "This confirms our intuition and shows that while the market is highly complex and "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
