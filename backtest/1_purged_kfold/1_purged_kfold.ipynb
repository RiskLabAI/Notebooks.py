{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea2b188c",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; width: 100%;\">  \n",
    "  <div style=\"display: flex; flex-direction: column; align-items: center; justify-content: center; width: 100px; margin-right: 20px;\">    \n",
    "    <a href=\"https://risklab.ai\" style=\"border: 0; line-height: 1;\">\n",
    "      <img src=\"../../utils/risklab_ai.gif\" width=\"100px\" style=\"border: 0; vertical-align: middle;\"/>\n",
    "    </a>\n",
    "  </div>  \n",
    "  <div style=\"flex-grow: 1;\">\n",
    "    <h1 style=\"margin: 0; font-weight: bold; text-align: left; font-size: 38px;\">\n",
    "      Cross-Validation using Purged K-fold\n",
    "    </h1>\n",
    "  </div>  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the `PurgedKFold` cross-validator from the `RiskLabAI` library to run a robust financial backtest. \n",
    "\n",
    "Standard `KFold` cross-validation fails in finance because:\n",
    "1.  **Leakage:** The training set can contain information that overlaps with the test set (e.g., from overlapping triple-barrier labels).\n",
    "2.  **Embargo:** Information from the test set can 'leak' back into the training set that follows it.\n",
    "\n",
    "`PurgedKFold` (De Prado, Chapter 7) solves this by:\n",
    "1.  **Purging:** Removing any training samples whose labels are concurrent with test set samples.\n",
    "2.  **Embargoing:** Removing training samples that immediately *follow* the test set to prevent look-ahead bias.\n",
    "\n",
    "We will:\n",
    "1.  Load data and generate features/labels.\n",
    "2.  Define the `event_times` (`t1`) needed for purging.\n",
    "3.  Instantiate and visualize `PurgedKFold`.\n",
    "4.  Run a full backtest loop manually (for teaching).\n",
    "5.  Run a full backtest using the optimized `backtest_predictions` method (recommended)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Third-party for data and modeling\n",
    "import yfinance as yf\n",
    "import ta\n",
    "from ta.utils import dropna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# RiskLabAI Imports\n",
    "# --- NEW: Import the controller --- \n",
    "from RiskLabAI.backtest.validation import CrossValidatorController\n",
    "import RiskLabAI.utils.publication_plots as pub_plots\n",
    "\n",
    "# Setup plotting and configuration\n",
    "pub_plots.setup_publication_style()\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2e404",
   "metadata": {},
   "source": [
    "## 1. Load Data & Generate Signals\n",
    "\n",
    "We will load 'SPY' data and generate some simple features and labels. For this example, we will assume a simple fixed-time horizon label (e.g., 20 days), so our `event_times` (`t1`) will be 20 days after the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16844281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import ta\n",
    "from ta.utils import dropna\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "ticker_symbol = \"SPY\"\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = \"2025-01-01\"\n",
    "\n",
    "data = yf.Ticker(ticker_symbol).history(\n",
    "    start=start_date, \n",
    "    end=end_date, \n",
    "    auto_adjust=True\n",
    ")\n",
    "\n",
    "# --- CRITICAL FIX: Remove timezone information ---\n",
    "# yfinance returns tz-aware indices, which conflict with tz-naive DateOffsets.\n",
    "data.index = data.index.tz_localize(None)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# --- 2. Generate Features (X) ---\n",
    "X = pd.DataFrame(index=data.index)\n",
    "X['rsi'] = ta.momentum.RSIIndicator(data['Close'], window=14).rsi()\n",
    "X['roc'] = ta.momentum.ROCIndicator(data['Close'], window=10).roc()\n",
    "\n",
    "# --- 3. Generate Labels (y) ---\n",
    "look_forward = 20\n",
    "y = data['Close'].pct_change(look_forward).shift(-look_forward)\n",
    "y = pd.Series((y > 0).astype(int), name='label')\n",
    "\n",
    "# --- 4. Define Event Times (t1) ---\n",
    "# The label's 'end time' is 20 days after the observation.\n",
    "event_times = pd.Series(y.index, index=y.index).apply(lambda x: x + pd.DateOffset(days=look_forward))\n",
    "\n",
    "# --- 5. Align Data ---\n",
    "# Combine all data and drop NaNs to ensure X, y, and t1 are aligned\n",
    "all_data = pd.concat([X, y, event_times.rename('t1')], axis=1).dropna()\n",
    "\n",
    "X = all_data.drop(columns=['label', 't1'])\n",
    "y = all_data['label']\n",
    "event_times = all_data['t1']\n",
    "\n",
    "print(f\"--- SPY (ETF) Data Loaded (TZ-Naive) ---\")\n",
    "print(f\"Aligned data shapes: X={X.shape}, y={y.shape}, t1={event_times.shape}\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing Purged K-Fold Splits\n",
    "\n",
    "Before we run the backtest, let's visualize how `PurgedKFold` creates its splits. We'll use a `n_splits=10`.\n",
    "\n",
    "Notice how the **Train** (blue) bar is 'purged' (stops early) before the **Test** (red) bar begins, and an 'embargo' (a gap) is left after the test bar ends. This prevents any information leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "embargo_pct = 0.01 # 1% embargo\n",
    "\n",
    "# --- NEW: Use the CrossValidatorController for easy instantiation ---\n",
    "cv_controller = CrossValidatorController(\n",
    "    validator_type='purgedkfold',\n",
    "    n_splits=n_splits,\n",
    "    times=event_times,\n",
    "    embargo=embargo_pct\n",
    ")\n",
    "cv = cv_controller.get_validator()\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# --- Plotting the CV splits --- \n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "fold_indices = range(n_splits)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(cv.split(X)):\n",
    "    # Plot training indices\n",
    "    ax.fill_betweenx(\n",
    "        [i-0.4, i+0.4], \n",
    "        train_idx.min(), \n",
    "        train_idx.max(), \n",
    "        color='#00aedb', \n",
    "        alpha=0.7, \n",
    "        label='Train' if i == 0 else \"_nolegend_\"\n",
    "    )\n",
    "    \n",
    "    # Plot testing indices\n",
    "    ax.fill_betweenx(\n",
    "        [i-0.4, i+0.4], \n",
    "        test_idx.min(), \n",
    "        test_idx.max(), \n",
    "        color='#d11141', \n",
    "        alpha=0.9, \n",
    "        label='Test' if i == 0 else \"_nolegend_\"\n",
    "    )\n",
    "\n",
    "ax.set_yticks(fold_indices)\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in fold_indices])\n",
    "ax.set_xlabel('Index Location (iloc)')\n",
    "ax.set_title(f'Purged K-Fold (n_splits={n_splits}, embargo={embargo_pct})')\n",
    "ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backtesting with a Manual Loop (Tutorial Method)\n",
    "\n",
    "This is the core of the tutorial. We loop through each (train, test) split generated by `cv.split()`:\n",
    "1.  Train a `RandomForestClassifier` on the **training data**.\n",
    "2.  Generate predictions on the **testing data**.\n",
    "3.  Store these out-of-sample (OOS) predictions.\n",
    "\n",
    "After the loop, we will have a complete set of OOS predictions for our entire dataset, which we can then score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "oos_predictions_manual = pd.Series(index=y.index, dtype=float)\n",
    "\n",
    "print(\"Running manual backtest loop...\")\n",
    "\n",
    "# 1. Create the loop iterator\n",
    "fold_iterator = cv.split(X)\n",
    "\n",
    "# 2. Wrap the iterator with tqdm\n",
    "progress_bar = tqdm(enumerate(fold_iterator), total=n_splits, desc=\"Backtest Fold\")\n",
    "\n",
    "# 3. Loop as normal\n",
    "for fold, (train_idx, test_idx) in progress_bar:\n",
    "    # Get data for this fold\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_test = X.iloc[test_idx]\n",
    "    \n",
    "    # 1. Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 2. Generate predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 3. Store OOS predictions\n",
    "    oos_predictions_manual.iloc[test_idx] = y_pred\n",
    "\n",
    "    # --- NEW: Update the progress bar's description ---\n",
    "    progress_bar.set_postfix({\n",
    "        \"Train Samples\": len(X_train),\n",
    "        \"Test Samples\": len(X_test)\n",
    "    })\n",
    "\n",
    "print(\"\\nBacktest complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate OOS Accuracy\n",
    "\n",
    "Now we can compare our complete set of OOS predictions against the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs from predictions (for any indices that were not in a test set)\n",
    "oos_predictions_manual = oos_predictions_manual.dropna()\n",
    "y_true_manual = y.loc[oos_predictions_manual.index]\n",
    "\n",
    "accuracy_manual = accuracy_score(y_true_manual, oos_predictions_manual)\n",
    "\n",
    "print(f\"Manual Loop OOS Accuracy: {accuracy_manual * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backtesting with `backtest_predictions`\n",
    "\n",
    "The manual loop above is good for understanding the *process*, but your `RiskLabAI` library provides a much faster, optimized, and parallelized method: `backtest_predictions`.\n",
    "\n",
    "This single function call does the entire loop for you, running the model training in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running optimized backtest using 'backtest_predictions'...\")\n",
    "\n",
    "# 1. Define a new model instance\n",
    "model_optimized = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 2. Run the backtest (this handles the loop + parallelization)\n",
    "path_predictions = cv.backtest_predictions(\n",
    "    estimator=model_optimized,\n",
    "    data=X,\n",
    "    labels=y,\n",
    "    n_jobs=-1 # Use all available CPU cores\n",
    ")\n",
    "\n",
    "print(\"Optimized backtest complete.\")\n",
    "\n",
    "# For standard K-Fold, predictions are in 'Path 1'\n",
    "oos_predictions_optimized = pd.Series(path_predictions['Path 1'], index=X.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the optimized results\n",
    "oos_predictions_optimized = oos_predictions_optimized.dropna()\n",
    "y_true_optimized = y.loc[oos_predictions_optimized.index]\n",
    "\n",
    "accuracy_optimized = accuracy_score(y_true_optimized, oos_predictions_optimized)\n",
    "\n",
    "print(f\"Optimized Method OOS Accuracy: {accuracy_optimized * 100:.2f}%\")\n",
    "\n",
    "# 5. Verify results are identical\n",
    "if np.isclose(accuracy_manual, accuracy_optimized):\n",
    "    print(\"\\nSuccess! Both methods produced the same result.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Results do not match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Getting Path Indices\n",
    "\n",
    "If you just want the *indices* for all splits (like the ones we plotted) without running predictions, you can use the `backtest_paths` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = cv.backtest_paths(X)\n",
    "\n",
    "print(f\"Path Keys: {paths.keys()}\")\n",
    "print(f\"Number of splits in 'Path 1': {len(paths['Path 1'])}\")\n",
    "print(\"\\nExample (first split, 'Train' indices):\")\n",
    "print(paths['Path 1'][0]['Train'][:10])\n",
    "print(\"\\nExample (first split, 'Test' indices):\")\n",
    "print(paths['Path 1'][0]['Test'][:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risklab311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
