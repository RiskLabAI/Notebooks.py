{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our research study where we delve into the intricate world of financial analytics. The following abstract outlines our exploration of advanced statistical models and machine learning techniques, marking a departure from traditional analytical methods. Our work focuses on the critical evaluation of model robustness and introduces specialized cross-validation techniques suitable for the financial domain. The insights presented aim to bridge the gap between theoretical finance and practical application, ensuring the reliability of financial models in complex market environments.\n",
    "\n",
    "### Abstract\n",
    "This research explores the integration of advanced statistical models and machine learning in financial analytics, representing a shift from traditional to advanced, data-driven methods. We address a critical gap in quantitative finance: the need for robust model evaluation and out-of-sample testing methodologies, particularly tailored cross-validation techniques for financial markets. We present a comprehensive framework to assess these methods, considering the unique characteristics of financial data like non-stationarity, autocorrelation, and regime shifts. Through our analysis, we unveil the marked superiority of the Combinatorial Purged (CPCV) method in mitigating overfitting risks, outperforming traditional methods like K-Fold, Purged K-Fold, and especially Walk-Forward, as evidenced by its lower Probability of Backtest Overfitting (PBO) and superior Deflated Sharpe Ratio (DSR) Test Statistic. Walk-Forward, by contrast, exhibits notable shortcomings in false discovery prevention, characterized by increased temporal variability and weaker stationarity. This contrasts starkly with CPCV's demonstrable stability and efficiency, confirming its reliability for financial strategy development. The analysis also suggests that choosing between Purged K-Fold and K-Fold necessitates caution due to their comparable performance and potential impact on the robustness of training data in out-of-sample testing. Our investigation utilizes a Synthetic Controlled Environment incorporating advanced models like the Heston Stochastic Volatility, Merton Jump Diffusion, and Drift-Burst Hypothesis, alongside regime-switching models. This approach provides a nuanced simulation of market conditions, offering new insights into evaluating cross-validation techniques. Our study underscores the necessity of specialized validation methods in financial modeling, especially in the face of growing regulatory demands and complex market dynamics. It bridges theoretical and practical finance, offering a fresh outlook on financial model validation. Highlighting the significance of advanced cross-validation techniques like CPCV, our research enhances the reliability and applicability of financial models in decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Modules\n",
    "In this section, we are importing the necessary libraries and modules required for our financial analytics. We have standard libraries such as `numpy` and `pandas` for data manipulation, and we import various machine learning models and tools from `scikit-learn` and `xgboost`. Additionally, we utilize `joblib` for improving the performance of our models. The `RiskLabAI` package provides us with synthetic data generation and backtesting functionality, which is crucial for our analysis of financial models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import Parallel, delayed\n",
    "from joblib_progress import joblib_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RiskLabAI.data.synthetic_data import drift_volatility_burst, parallel_generate_prices\n",
    "from RiskLabAI.backtest import backtset_overfitting_simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Parameters\n",
    "Here we define the simulation parameters such as the number of jobs (`N_JOBS`), paths (`N_PATHS`), and the total time (`TOTAL_TIME`). These parameters will be used to create a synthetic controlled environment that simulates market conditions, allowing us to evaluate our cross-validation techniques effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = 24\n",
    "N_PATHS = 1000\n",
    "TOTAL_TIME = 40\n",
    "N_STEPS = int(252 * TOTAL_TIME)\n",
    "RISK_FREE_RATE = 0.05\n",
    "STEP_RISK_FREE_RATE = np.log(1 + RISK_FREE_RATE) / N_STEPS * TOTAL_TIME\n",
    "RANDOM_STATE = 0\n",
    "OVERFITTING_PARTITIONS_LENGTH = 252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Regime Parameters and Custom Pipeline\n",
    "We define parameters for market regimes, using a `drift_volatility_burst` function to simulate market conditions, including calm, volatile, and speculative bubble regimes. These regimes are characterized by specific Heston model parameters such as mean return (`mu`), rate at which variance reverts to theta (`kappa`), long-run average price variance (`theta`), and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.35\n",
    "\n",
    "bubble_drifts, bubble_volatilities = drift_volatility_burst(\n",
    "    bubble_length=5 * 252, \n",
    "    a_before=x, \n",
    "    a_after=-x, \n",
    "    b_before=0.6 * x, \n",
    "    b_after=0.6 * x, \n",
    "    alpha=0.75, \n",
    "    beta=0.45,\n",
    "    explosion_filter_width=0.1\n",
    ")\n",
    "# Dictionary of Heston parameters for different market regimes\n",
    "regimes = {\n",
    "    'calm': {\n",
    "        'mu': 0.1,\n",
    "        'kappa': 3.98,\n",
    "        'theta': 0.029,\n",
    "        'xi': 0.389645311,\n",
    "        'rho': -0.7,\n",
    "        'lam': 121,\n",
    "        'm': -0.000709,\n",
    "        'v': 0.0119\n",
    "    },\n",
    "    'volatile': {\n",
    "        'mu': 0.1,\n",
    "        'kappa': 3.81,\n",
    "        'theta': 0.25056,\n",
    "        'xi': 0.59176974,\n",
    "        'rho': -0.7,\n",
    "        'lam': 121,\n",
    "        'm': -0.000709,\n",
    "        'v': 0.0119\n",
    "    },\n",
    "    'speculative_bubble': {\n",
    "        'mu': list(bubble_drifts),\n",
    "        'kappa': 1,\n",
    "        'theta': list(bubble_volatilities),\n",
    "        'xi': 0,\n",
    "        'rho': 0,\n",
    "        'lam': 0,\n",
    "        'm': 0,\n",
    "        'v': 0.00000001\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPipeline(Pipeline):\n",
    "    @classmethod\n",
    "    def from_existing_pipeline(cls, existing_pipeline, memory=None, verbose=False):\n",
    "        return cls(steps=existing_pipeline.steps, memory=memory, verbose=verbose)\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        if 'sample_weight' in fit_params:\n",
    "            sample_weight = fit_params.pop('sample_weight')\n",
    "            for step_name, _ in self.steps:\n",
    "                fit_params[f\"{step_name}__sample_weight\"] = sample_weight\n",
    "        return super().fit(X, y, **fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Matrix and Strategy Parameters\n",
    "A transition matrix is established to represent the probability of transitioning between market states. Additionally, we define `strategy_parameters` for various trading strategies to be tested in our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = TOTAL_TIME / N_STEPS\n",
    "\n",
    "transition_matrix = np.array([\n",
    "    [1 - 1 * dt,   1 * dt - 0.00001,        0.00001],  # State 0 transitions\n",
    "    [20 * dt,      1 - 20 * dt - 0.00001,   0.00001],  # State 1 transitions\n",
    "    [1 - 1 * dt,   1 * dt,                      0.0],  # State 2 transitions\n",
    "])\n",
    "\n",
    "strategy_parameters = {\n",
    "    'fast_window' : [5, 20, 50, 70],\n",
    "    'slow_window' : [10, 50, 100, 140],\n",
    "    'exponential' : [False],\n",
    "    'mean_reversion' : [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Parameter Grids\n",
    "We declare a dictionary of machine learning models, including k-Nearest Neighbors (k-NN), Decision Tree, and XGBoost. For each model, we specify a custom pipeline and a grid of hyperparameters to be optimized during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "\n",
    "models = {\n",
    "    'k-NN' : {\n",
    "        'Model': CustomPipeline.from_existing_pipeline(existing_pipeline=make_pipeline(StandardScaler(), KNeighborsClassifier())),\n",
    "        'Parameters': {\n",
    "            'kneighborsclassifier__n_neighbors': [1, 2, 3],\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree' : {\n",
    "        'Model': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "        'Parameters': {\n",
    "            'max_depth': [None],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [1],\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'Model': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', seed=RANDOM_STATE),\n",
    "        'Parameters': {\n",
    "            'n_estimators': [1000],\n",
    "            'max_depth': [1000000000],\n",
    "            'learning_rate': [1, 10, 100],\n",
    "            'subsample': [1.0],\n",
    "            'colsample_bytree': [1.0],\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing Prices\n",
    "Finally, we use the `parallel_generate_prices` function to synthesize asset prices for different market regimes, which will serve as the dataset for our backtesting and model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Synthesizing Prices...')\n",
    "all_prices, all_regimes = parallel_generate_prices(\n",
    "    N_PATHS,\n",
    "    regimes,\n",
    "    transition_matrix,\n",
    "    TOTAL_TIME,\n",
    "    N_STEPS,\n",
    "    RANDOM_STATE,\n",
    "    N_JOBS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Evaluation\n",
    "The code block executes a parallelized backtesting simulation on all price columns to evaluate the risk of overfitting in different cross-validation (CV) methods. Results from the simulation are collected into lists, which are then transformed into DataFrames for more detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib_progress(\"Overfitting...\", total=all_prices.shape[1]):    \n",
    "    results = Parallel(n_jobs=N_JOBS)(delayed(backtset_overfitting_simulation)(all_prices[column], strategy_parameters, models, STEP_RISK_FREE_RATE, all_prices.shape[0]) for column in all_prices.columns)\n",
    "\n",
    "# Assuming results is already populated from the joblib Parallel call\n",
    "cv_pbo_list = [result[0] for result in results]  # Collect all cv_pbo dicts\n",
    "cv_deflated_sr_list = [result[1] for result in results]  # Collect all cv_deflated_sr dicts\n",
    "\n",
    "# Initialize dicts to collect lists for each CV method\n",
    "cv_pbo_data = {cv: [] for cv in cv_pbo_list[0].keys()}\n",
    "cv_deflated_sr_data = {cv: [] for cv in cv_deflated_sr_list[0].keys()}\n",
    "\n",
    "# Populate the cv_pbo_data and cv_deflated_sr_data with concatenated lists from each result\n",
    "for cv_pbo in cv_pbo_list:\n",
    "    for cv, values in cv_pbo.items():\n",
    "        cv_pbo_data[cv].append(values)\n",
    "\n",
    "for cv_deflated_sr in cv_deflated_sr_list:\n",
    "    for cv, values in cv_deflated_sr.items():\n",
    "        cv_deflated_sr_data[cv].append(values)\n",
    "\n",
    "# Convert the collected lists into DataFrames\n",
    "cv_pbo_dfs = {cv: pd.DataFrame(cv_pbo_data[cv]).T for cv in cv_pbo_data}\n",
    "cv_deflated_sr_dfs = {cv: pd.DataFrame(cv_deflated_sr_data[cv]).T for cv in cv_deflated_sr_data}\n",
    "\n",
    "# Mapping from descriptive CV names to filesystem-friendly names\n",
    "cv_name_map = {\n",
    "    'Walk-Forward': 'walkforward',\n",
    "    'K-Fold': 'kfold',\n",
    "    'Purged K-Fold': 'purgedkfold',\n",
    "    'Combinatorial Purged': 'combinatorialpurged',\n",
    "}\n",
    "\n",
    "# Save each cv_pbo DataFrame to CSV using the mapping for file names\n",
    "for cv_name, df in cv_pbo_dfs.items():\n",
    "    file_name = cv_name_map.get(cv_name, cv_name)  # Fallback to cv_name if not found in the map\n",
    "    df.to_csv(f'overall_simulated_pbo_{file_name}.csv', index=False)\n",
    "\n",
    "# Save each cv_deflated_sr DataFrame to CSV using the mapping for file names\n",
    "for cv_name, df in cv_deflated_sr_dfs.items():\n",
    "    file_name = cv_name_map.get(cv_name, cv_name)  # Fallback to cv_name if not found in the map\n",
    "    df.to_csv(f'overall_simulated_deflated_sr_{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioned Data Evaluation\n",
    "In a temporal analysis, the same simulation is conducted on partitions of the data to understand how each CV method performs over time. The results are stored in CSV files for each CV method, mapping descriptive names to filesystem-friendly names for consistency and ease of access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with joblib_progress(\"Overfitting...\", total=all_prices.shape[1]):    \n",
    "    results = Parallel(n_jobs=N_JOBS)(delayed(backtset_overfitting_simulation)(all_prices[column], strategy_parameters, models, STEP_RISK_FREE_RATE, OVERFITTING_PARTITIONS_LENGTH) for column in all_prices.columns)\n",
    "\n",
    "# Assuming results is already populated from the joblib Parallel call\n",
    "cv_pbo_list = [result[0] for result in results]  # Collect all cv_pbo dicts\n",
    "cv_deflated_sr_list = [result[1] for result in results]  # Collect all cv_deflated_sr dicts\n",
    "\n",
    "# Initialize dicts to collect lists for each CV method\n",
    "cv_pbo_data = {cv: [] for cv in cv_pbo_list[0].keys()}\n",
    "cv_deflated_sr_data = {cv: [] for cv in cv_deflated_sr_list[0].keys()}\n",
    "\n",
    "# Populate the cv_pbo_data and cv_deflated_sr_data with concatenated lists from each result\n",
    "for cv_pbo in cv_pbo_list:\n",
    "    for cv, values in cv_pbo.items():\n",
    "        cv_pbo_data[cv].append(values)\n",
    "\n",
    "for cv_deflated_sr in cv_deflated_sr_list:\n",
    "    for cv, values in cv_deflated_sr.items():\n",
    "        cv_deflated_sr_data[cv].append(values)\n",
    "\n",
    "# Convert the collected lists into DataFrames\n",
    "cv_pbo_dfs = {cv: pd.DataFrame(cv_pbo_data[cv]).T for cv in cv_pbo_data}\n",
    "cv_deflated_sr_dfs = {cv: pd.DataFrame(cv_deflated_sr_data[cv]).T for cv in cv_deflated_sr_data}\n",
    "\n",
    "# Mapping from descriptive CV names to filesystem-friendly names\n",
    "cv_name_map = {\n",
    "    'Walk-Forward': 'walkforward',\n",
    "    'K-Fold': 'kfold',\n",
    "    'Purged K-Fold': 'purgedkfold',\n",
    "    'Combinatorial Purged': 'combinatorialpurged',\n",
    "}\n",
    "\n",
    "# Save each cv_pbo DataFrame to CSV using the mapping for file names\n",
    "for cv_name, df in cv_pbo_dfs.items():\n",
    "    file_name = cv_name_map.get(cv_name, cv_name)  # Fallback to cv_name if not found in the map\n",
    "    df.to_csv(f'simulated_pbo_{file_name}.csv', index=False)\n",
    "\n",
    "# Save each cv_deflated_sr DataFrame to CSV using the mapping for file names\n",
    "for cv_name, df in cv_deflated_sr_dfs.items():\n",
    "    file_name = cv_name_map.get(cv_name, cv_name)  # Fallback to cv_name if not found in the map\n",
    "    df.to_csv(f'simulated_deflated_sr_{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By conducting these simulations, we can quantify the Probability of Backtest Overfitting (PBO) and the performance of the Deflated Sharpe Ratio (DSR) across different CV methods, providing a comprehensive view of their robustness in temporal contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
